{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/Bio/Application/__init__.py:39: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.\n",
      "\n",
      "Due to the on going maintenance burden of keeping command line application\n",
      "wrappers up to date, we have decided to deprecate and eventually remove these\n",
      "modules.\n",
      "\n",
      "We instead now recommend building your command line and invoking it directly\n",
      "with the subprocess module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils import gc_fraction\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Align.Applications import ClustalwCommandline\n",
    "import collections\n",
    "from Bio import AlignIO\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#sys.path.append(os.path.abspath('/home/nimar/PLP_directRNA_design_V2/PLP_directRNA_design/'))\n",
    "#from PLP_directRNA_design import probedesign as plp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting sequence from gtf/fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transcripts_biopython(genome_fasta, annotation_gtf, output_fasta):\n",
    "    \"\"\"\n",
    "    Extracts coding sequences (CDS) of transcripts using Biopython.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load genome sequences\n",
    "    genome = SeqIO.to_dict(SeqIO.parse(genome_fasta, \"fasta\"))\n",
    "    \n",
    "    # Create GTF database\n",
    "    db = gffutils.create_db(annotation_gtf, dbfn='annotation.db', force=True, keep_order=True, \n",
    "                            merge_strategy=\"merge\", disable_infer_genes=True)\n",
    "    \n",
    "    transcripts = defaultdict(str)\n",
    "    \n",
    "    # Extract CDS sequences for each transcript\n",
    "    for transcript in db.features_of_type(\"transcript\"):\n",
    "        chrom = transcript.seqid\n",
    "        cds_seq = \"\"\n",
    "        \n",
    "        for cds in db.children(transcript, featuretype=\"CDS\", order_by=\"start\"):\n",
    "            start = cds.start - 1  # Convert to 0-based index\n",
    "            end = cds.end\n",
    "            \n",
    "            if chrom in genome:\n",
    "                seq_fragment = genome[chrom].seq[start:end]\n",
    "                if cds.strand == \"-\":\n",
    "                    seq_fragment = seq_fragment.reverse_complement()\n",
    "                cds_seq += str(seq_fragment)\n",
    "        \n",
    "        if cds_seq:\n",
    "            transcripts[transcript.id] = cds_seq\n",
    "    \n",
    "    # Write output\n",
    "    with open(output_fasta, \"w\") as f:\n",
    "        for tid, seq in transcripts.items():\n",
    "            f.write(f\">{tid}\\n{seq}\\n\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Biopython CDS extraction completed in {elapsed_time:.2f} seconds. Output saved to: {output_fasta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173 of 174 (99%)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biopython CDS extraction completed in 4.20 seconds. Output saved to: tmp.fasta\n"
     ]
    }
   ],
   "source": [
    "extract_transcripts_biopython('../data/Mus.fa', annotation_gtf='../data/tmp.gtf', output_fasta='tmp.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melting temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oligo designer toolsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#targets_df = pd.read_csv('../targets.txt', sep='\\t')\n",
    "#targets_df.set_index('Probe_id', inplace=True)\n",
    "specificity_results = pd.read_csv('../probes_check.csv', sep=',')\n",
    "selected_features = '../extract_features_output.txt'\n",
    "#print(selected_features.head())\n",
    "\n",
    "targets_df = targets_df_check = find_targets(\n",
    "    selected_features=selected_features,\n",
    "    fasta_file='../extract_seqs_output.fa',\n",
    "    reference_fasta=\"../data/tmp_mrna.fa\",\n",
    "    plp_length=30,\n",
    "    min_coverage=1,\n",
    "    gc_min=50, gc_max=60,\n",
    "    max_errors=1, check_specificity=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install oligo-designer-toolsuite\n",
    "from oligo_designer_toolsuite.oligo_efficiency_filter import WeightedTmGCOligoScoring\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from oligo_designer_toolsuite.database import (\n",
    "    OligoAttributes,\n",
    "    OligoDatabase,\n",
    ")\n",
    "# Define scoring parameters\n",
    "Tm_min = 55.0  # Minimum acceptable Tm\n",
    "Tm_opt = 60.0  # Optimal Tm\n",
    "Tm_max = 65.0  # Maximum acceptable Tm\n",
    "GC_content_opt = 50.0  # Optimal GC content percentage\n",
    "\n",
    "# Initialize the scoring object\n",
    "scorer = WeightedTmGCOligoScoring(Tm_min=Tm_min, Tm_opt=Tm_opt, Tm_max=Tm_max, \n",
    "                                  GC_content_min = 50, GC_content_max = 60,\n",
    "                                  Tm_parameters={},\n",
    "                                  GC_content_opt=GC_content_opt)\n",
    "\n",
    "min_oligos_per_region = 3\n",
    "write_regions_with_insufficient_oligos = True\n",
    "n_jobs = 3\n",
    "lru_db_max_in_memory=n_jobs * 2 + 2\n",
    "database_name=\"db_oligos\"\n",
    "oligo_database = OligoDatabase(\n",
    "    min_oligos_per_region=min_oligos_per_region,\n",
    "    write_regions_with_insufficient_oligos=write_regions_with_insufficient_oligos,\n",
    "    lru_db_max_in_memory=lru_db_max_in_memory,\n",
    "    database_name=database_name,\n",
    "    dir_output='../oligo_database',\n",
    "    n_jobs=n_jobs,\n",
    ")\n",
    "\n",
    "# Assuming 'oligo_database' is your database of probes and 'region_id' specifies the target region\n",
    "#oligo_database, scores = scorer.apply(oligo_database, region_id='target_region')\n",
    "oligo_database.load_database_from_fasta('Probes.txttmp.fa',\n",
    "                                        database_overwrite=True,\n",
    "                                        sequence_type='target')\n",
    "fasta_file = 'Probes.txttmp.fa'\n",
    "fasta_records = SeqIO.parse(fasta_file, \"fasta\")\n",
    "probes_df = pd.DataFrame([{\n",
    "    \"id\": record.id,\n",
    "    \"sequence\": str(record.seq),\n",
    "    \"gc_content\": gc_fraction(record.seq),\n",
    "    \"scores\": int(0),\n",
    "} for record in fasta_records])\n",
    "probes_df.set_index('id', inplace=True)\n",
    "\n",
    "scores_list = []\n",
    "for record in probes_df.index:\n",
    "    oligo_database, scores = scorer.apply(oligo_database, region_id=record, sequence_type='target')\n",
    "    print(scores[0])\n",
    "    scores_list.append(scores[0])  # Assuming scores[0] contains the relevant score\n",
    "\n",
    "probes_df['scores'][:8] = scores_list\n",
    "probes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python function based on `WeightedTmGCOligoScoring`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.SeqUtils import gc_fraction\n",
    "\n",
    "def weighted_tm_gc_scoring(sequence, Tm_oligo, Tm_min=55.0, Tm_opt=60.0, Tm_max=65.0, \n",
    "                           GC_min=40.0, GC_opt=50.0, GC_max=60.0, w_Tm=1.0, w_GC=1.0):\n",
    "    \"\"\"\n",
    "    Compute the weighted score for a given oligo based on melting temperature and GC content.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequence (str): The nucleotide sequence of the oligo.\n",
    "    - Tm_oligo (float): The computed melting temperature of the oligo.\n",
    "    - Tm_min, Tm_opt, Tm_max (float): Min, optimal, and max melting temperature thresholds.\n",
    "    - GC_min, GC_opt, GC_max (float): Min, optimal, and max GC content thresholds.\n",
    "    - w_Tm, w_GC (float): Weights for the Tm and GC components of the score.\n",
    "\n",
    "    Returns:\n",
    "    - float: The weighted score (lower is better).\n",
    "    \"\"\"\n",
    "    # Compute GC content\n",
    "    GC_oligo = gc_fraction(sequence) * 100  # Convert fraction to percentage\n",
    "\n",
    "    # Compute Tm deviation score\n",
    "    if Tm_oligo >= Tm_opt:\n",
    "        score_Tm = abs(Tm_oligo - Tm_opt) / (Tm_max - Tm_opt)\n",
    "    else:\n",
    "        score_Tm = abs(Tm_oligo - Tm_opt) / (Tm_opt - Tm_min)\n",
    "    \n",
    "    # Compute GC deviation score\n",
    "    if GC_oligo >= GC_opt:\n",
    "        score_GC = abs(GC_oligo - GC_opt) / (GC_max - GC_opt)\n",
    "    else:\n",
    "        score_GC = abs(GC_oligo - GC_opt) / (GC_opt - GC_min)\n",
    "    \n",
    "    # Compute final weighted score\n",
    "    score = w_Tm * score_Tm + w_GC * score_GC\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = []\n",
    "for record in probes_df.index:\n",
    "    oligo_seq = probes_df.loc[record]['sequence']\n",
    "\n",
    "    # Compute Tm using nearest-neighbor method\n",
    "    Tm_oligo = mt.Tm_NN(oligo_seq)\n",
    "\n",
    "    # Compute the weighted score\n",
    "    score = weighted_tm_gc_scoring(oligo_seq, Tm_oligo)\n",
    "    scores_list.append(score)  # Assuming scores[0] contains the relevant score\n",
    "\n",
    "probes_df['scores_python'] = scores_list\n",
    "\n",
    "# calculate correlation between the two scores\n",
    "probes_df[['scores', 'scores_python']][:8].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing for each arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio.SeqUtils import gc_fraction, MeltingTemp as mt\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "def weighted_tm_gc_scoring(sequence, Tm_oligo, Tm_min=55.0, Tm_opt=60.0, Tm_max=65.0, \n",
    "                           GC_min=40.0, GC_opt=50.0, GC_max=60.0, w_Tm=1.0, w_GC=1.0):\n",
    "    \"\"\"Compute the weighted score for a given oligo based on melting temperature and GC content.\"\"\"\n",
    "    GC_oligo = gc_fraction(sequence) * 100  # Convert fraction to percentage\n",
    "\n",
    "    # Compute Tm deviation score\n",
    "    if Tm_oligo >= Tm_opt:\n",
    "        score_Tm = abs(Tm_oligo - Tm_opt) / (Tm_max - Tm_opt)\n",
    "    else:\n",
    "        score_Tm = abs(Tm_oligo - Tm_opt) / (Tm_opt - Tm_min)\n",
    "    \n",
    "    # Compute GC deviation score\n",
    "    if GC_oligo >= GC_opt:\n",
    "        score_GC = abs(GC_oligo - GC_opt) / (GC_max - GC_opt)\n",
    "    else:\n",
    "        score_GC = abs(GC_oligo - GC_opt) / (GC_opt - GC_min)\n",
    "    \n",
    "    return w_Tm * score_Tm + w_GC * score_GC  # Lower score is better\n",
    "\n",
    "\n",
    "def analyze_scores(scores, percentile=5):\n",
    "    \"\"\"Analyze the score distribution and suggest a cutoff.\"\"\"\n",
    "    suggested_cutoff = np.percentile(scores, percentile)  # Get the threshold for top X%\n",
    "    return suggested_cutoff\n",
    "\n",
    "\n",
    "def score_padlock_probe(sequence, Tm_min=55.0, Tm_opt=60.0, Tm_max=65.0, \n",
    "                        GC_min=40.0, GC_opt=50.0, GC_max=60.0, w_Tm=1.0, w_GC=1.0, percentile=5):\n",
    "    \"\"\"Score a padlock probe by splitting it into two arms and analyzing distribution.\"\"\"\n",
    "    mid = len(sequence) // 2\n",
    "    left_arm, right_arm = sequence[:mid], sequence[mid:]\n",
    "\n",
    "    # Compute Tm for each arm\n",
    "    Tm_left = mt.Tm_NN(left_arm)\n",
    "    Tm_right = mt.Tm_NN(right_arm)\n",
    "\n",
    "    # Score each arm separately\n",
    "    score_left = weighted_tm_gc_scoring(left_arm, Tm_left, Tm_min, Tm_opt, Tm_max, GC_min, GC_opt, GC_max, w_Tm, w_GC)\n",
    "    score_right = weighted_tm_gc_scoring(right_arm, Tm_right, Tm_min, Tm_opt, Tm_max, GC_min, GC_opt, GC_max, w_Tm, w_GC)\n",
    "\n",
    "    # Combine scores (you can take the average, max, or another approach)\n",
    "    final_score = (score_left + score_right) / 2  # Averaging the scores\n",
    "\n",
    "    return final_score\n",
    "\n",
    "\n",
    "def visualize_score_distribution(scores, cutoff):\n",
    "    \"\"\"Plot the score distribution and cutoff.\"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(scores, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.axvline(cutoff, color='red', linestyle='dashed', linewidth=2, label=f'Cutoff ({cutoff:.2f})')\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Probe Scores\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing final selecttion of the probes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()\n",
    "targets_df = pd.read_csv('../tmp_targets.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking the cutadapt results in the stable version (20250407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pandas import DataFrame\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils import gc_fraction\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import collections\n",
    "from Bio import AlignIO\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re\n",
    "import os \n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import itertools\n",
    "from tqdm import tqdm  \n",
    "import matplotlib.pyplot as plt\n",
    "from Bio.SeqUtils import gc_fraction, MeltingTemp as mt\n",
    "\n",
    "# Dictionaries\n",
    "\n",
    "## IUPAC dictionary\n",
    "IUPAC_CODES = {\n",
    "    \"R\": [\"A\", \"G\"],\n",
    "    \"Y\": [\"C\", \"T\"],\n",
    "    \"S\": [\"G\", \"C\"],\n",
    "    \"W\": [\"A\", \"T\"],\n",
    "    \"K\": [\"G\", \"T\"],\n",
    "    \"M\": [\"A\", \"C\"],\n",
    "    \"B\": [\"C\", \"G\", \"T\"],\n",
    "    \"D\": [\"A\", \"G\", \"T\"],\n",
    "    \"H\": [\"A\", \"C\", \"T\"],\n",
    "    \"V\": [\"A\", \"C\", \"G\"],\n",
    "    \"N\": [\"A\", \"C\", \"G\", \"T\"],  \n",
    "    \"A\": [\"A\"],\n",
    "    \"C\": [\"C\"],\n",
    "    \"G\": [\"G\"],\n",
    "    \"T\": [\"T\"]\n",
    "}\n",
    "\n",
    "## Ligation junction dictionary\n",
    "ligation_junctions_dict = {'TA': 'preferred',\n",
    "                        'TA': 'preferred',\n",
    "                        'GA': 'preferred',\n",
    "                        'AG': 'preferred',\n",
    "                        'TT': 'neutral',\n",
    "                        'CT': 'neutral',\n",
    "                        'CA': 'neutral',\n",
    "                        'TC': 'neutral',\n",
    "                        'AC': 'neutral',\n",
    "                        'CC': 'neutral',\n",
    "                        'TG': 'neutral',\n",
    "                        'AA': 'neutral', \n",
    "                        'CG': 'non-preferred', \n",
    "                        'GT': 'non-preferred',\n",
    "                        'GG': 'non-preferred',\n",
    "                        'GC': 'non-preferred'}\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "## Evaluate IUPAC mismatches format:\n",
    "def parse_iupac_mismatches(mismatch_str):\n",
    "    \"\"\"\n",
    "    Parses a string of mismatches formatted as \"pos:base,pos:base\" into a list of tuples.\n",
    "    \n",
    "    Args:\n",
    "        mismatch_str (str): Mismatch input string (e.g., \"5:R,10:G\")\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of (position, base) tuples, e.g., [(5, 'R'), (10, 'G')].\n",
    "    \"\"\"\n",
    "    mismatches = []\n",
    "    try:\n",
    "        for pair in mismatch_str.split(\",\"):\n",
    "            pos, base = pair.split(\":\")\n",
    "            pos = int(pos.strip())  # Convert position to integer\n",
    "            base = base.strip().upper()  # Ensure base is uppercase\n",
    "            mismatches.append((pos, base))\n",
    "\n",
    "    except (ValueError, IndexError):\n",
    "        raise InputValueError(\"Invalid format for --iupac_mismatches. Use 'pos:base,pos:base', e.g., '5:R,10:G'.\", \n",
    "                              field=\"iupac_mismatches\", code=\"invalid_mismatch_format\")\n",
    "    \n",
    "    return mismatches\n",
    "## Evaluate ligation junction functions:\n",
    "def evaluate_ligation_junction(targets, iupac_mismatches=None, plp_length=30):\n",
    "    \"\"\"\n",
    "    Evaluates the ligation junction of a probe and introduces mismatches if needed.\n",
    "\n",
    "    Args:\n",
    "        targets (pd.DataFrame): DataFrame containing probe sequences in the 'Sequence' column.\n",
    "        iupac_mismatches (str or list of tuples): Mismatch instructions in the form \"5:R,10:G\" or [(5, 'R'), (10, 'G')].\n",
    "        plp_length (int): Length of the probe (default: 30).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with modified probes and ligation junction statuses.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame has a 'Ligation junction' column\n",
    "    if 'Ligation junction' not in targets.columns:\n",
    "        targets['Ligation junction'] = 'non-preferred'\n",
    "\n",
    "    # Determine junction position (using the original probe indexing)\n",
    "    junction_position = int((plp_length / 2) - 1)\n",
    "    new_rows = []\n",
    "\n",
    "    for idx in targets.index:\n",
    "        probe_seq = targets.loc[idx]['Sequence']\n",
    "        ligation_junction = probe_seq[junction_position] + probe_seq[junction_position + 2]\n",
    "        ligation_status = ligation_junctions_dict.get(ligation_junction, \"non-preferred\")\n",
    "        targets.loc[idx, 'Ligation junction'] = ligation_status\n",
    "\n",
    "        if iupac_mismatches is not None:\n",
    "            # If mismatches are provided as a string, parse them into a list of (pos, symbol) tuples.\n",
    "            if isinstance(iupac_mismatches, str):\n",
    "                iupac_mismatches = parse_iupac_mismatches(iupac_mismatches)\n",
    "                \n",
    "            # Limit to 2 mismatches\n",
    "            if len(iupac_mismatches) > 2:\n",
    "                raise InputValueError(\"The number of mismatches should be less than or equal to 2\",\n",
    "                                      field=\"iupac_mismatches\", code=\"mismatches_exceed_limit\")\n",
    "\n",
    "            # Convert user-provided (1-indexed) positions to 0-indexed for internal use,\n",
    "            # while preserving the original 1-indexed value for the probe ID suffix.\n",
    "            # Each tuple becomes (adjusted_pos, original_pos, iupac_symbol)\n",
    "            mismatches_converted = [(pos - 1, pos, symbol) for pos, symbol in iupac_mismatches]\n",
    "\n",
    "            # Try all combinations of 1 or 2 mismatches\n",
    "            for r in range(1, len(mismatches_converted) + 1):\n",
    "                for subset in itertools.combinations(range(len(mismatches_converted)), r):\n",
    "                    selected_mismatches = [mismatches_converted[i] for i in subset]\n",
    "                    # For each mismatch, retrieve the possible replacement bases from IUPAC_CODES.\n",
    "                    replacement_options = [IUPAC_CODES[symbol] for _, _, symbol in selected_mismatches]\n",
    "\n",
    "                    # Generate all possible replacement combinations.\n",
    "                    for replacement in itertools.product(*replacement_options):\n",
    "                        original_seq_list = list(probe_seq)\n",
    "                        modified_seq = original_seq_list.copy()\n",
    "                        new_id_suffix = []\n",
    "                        changes_made = False\n",
    "\n",
    "                        for (adj_pos, orig_pos, iupac_symbol), new_base in zip(selected_mismatches, replacement):\n",
    "                            original_base = original_seq_list[adj_pos]\n",
    "                            # Apply the replacement only if it results in an actual change.\n",
    "                            if original_base != new_base:\n",
    "                                modified_seq[adj_pos] = new_base\n",
    "                                new_id_suffix.append(f\"{orig_pos}_{original_base}_{new_base}\")\n",
    "                                changes_made = True\n",
    "\n",
    "                        # Only add a new probe row if at least one change occurred.\n",
    "                        if changes_made:\n",
    "                            new_probe_seq = \"\".join(modified_seq)\n",
    "                            new_probe_id = f\"{targets.loc[idx, 'Probe_id']}|{'_'.join(new_id_suffix)}\"\n",
    "                            \n",
    "                            # Re-evaluate the ligation junction with the new sequence.\n",
    "                            new_ligation_junction = new_probe_seq[junction_position] + new_probe_seq[junction_position + 2]\n",
    "                            new_ligation_status = ligation_junctions_dict.get(new_ligation_junction, \"non-preferred\")\n",
    "\n",
    "                            new_row = targets.loc[idx].copy()\n",
    "                            new_row['Sequence'] = new_probe_seq\n",
    "                            new_row['Ligation junction'] = new_ligation_status\n",
    "                            new_row['Probe_id'] = new_probe_id\n",
    "\n",
    "                            new_rows.append((new_probe_id, new_row))\n",
    "\n",
    "    # Append the new rows to the DataFrame, if any.\n",
    "    if new_rows:\n",
    "        new_rows_df = pd.DataFrame([row[1] for row in new_rows], index=[row[0] for row in new_rows])\n",
    "        targets = pd.concat([targets, new_rows_df])\n",
    "\n",
    "    return targets\n",
    "\n",
    "## Custom exception classes:\n",
    "class InputValueError(ValueError):\n",
    "    \"\"\"\n",
    "    Custom exception class for invalid\n",
    "    input values.\n",
    "    \"\"\"\n",
    "    def __init__(self, message: str, field: str, code: str):\n",
    "        super().__init__(message)\n",
    "        self.field = field\n",
    "        self.code = code\n",
    "\n",
    "\n",
    "## Extract features functions:\n",
    "\n",
    "def parse_gtf_to_dataframe(gtf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parses a GTF (Gene Transfer Format) file into a structured Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        gtf_path (str): Path to the GTF file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing parsed GTF data.\n",
    "    \n",
    "    Reference:\n",
    "        Adapted from Ricardo Filipe dos Santos script:\n",
    "        https://gist.github.com/rf-santos/22f521c62ca2f85ac9582bf0d91e4054\n",
    "    \"\"\"\n",
    "    print('Loading GTF file....')\n",
    "    # Define column names\n",
    "    col_names = [\n",
    "        'seqname', 'source', 'feature', 'start', 'end', 'score', \n",
    "        'strand', 'frame', 'attribute'\n",
    "    ]\n",
    "    \n",
    "    dtype_dict = {\n",
    "        'seqname': str,\n",
    "        'source': str,\n",
    "        'feature': str,\n",
    "        'start': int,\n",
    "        'end': int,\n",
    "        'score': str,\n",
    "        'strand': str,\n",
    "        'frame': str,\n",
    "        'attribute': str\n",
    "    }\n",
    "    \n",
    "    # Read the GTF file, skipping comments\n",
    "    with open(gtf_path, 'r') as f:\n",
    "        cleaned_text = ''.join(line for line in f if not line.lstrip().startswith('#'))\n",
    "    buffer = StringIO(cleaned_text)\n",
    "\n",
    "    df = pd.read_csv(buffer, sep='\\t', header=None, names=col_names, dtype=dtype_dict)\n",
    "\n",
    "    # Define attributes to extract\n",
    "    attributes = [\n",
    "        'gene_id', 'transcript_id', 'exon_number', 'gene_name', 'gene_source', \n",
    "        'gene_biotype', 'transcript_name', 'transcript_source', 'transcript_biotype', \n",
    "        'protein_id', 'exon_id', 'tag'\n",
    "    ]\n",
    "\n",
    "    # Create regex patterns for attribute extraction\n",
    "    attr_patterns = {attr: rf'{attr} \"([^\"]*)\"' for attr in attributes}\n",
    "\n",
    "    # Extract attributes using vectorized operations\n",
    "    for attr, pattern in attr_patterns.items():\n",
    "        df[attr] = df['attribute'].str.extract(pattern)\n",
    "\n",
    "    # Drop the original attribute column\n",
    "    df.drop(columns=['attribute'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_gtf(gtf_file, genes_str=None, identifier_type='gene_id'):\n",
    "    \"\"\"\n",
    "    Parses a GTF file and yields data only for the specified genes_of_interest.\n",
    "\n",
    "    Args:\n",
    "        gtf_file (str): Path to the GTF file.\n",
    "        genes_of_interest (set or None): A set of gene IDs or names to parse.\n",
    "                                         If None, parse all genes in the GTF.\n",
    "        identifier_type (str): Type of identifier provided ('gene_id' or 'gene_name').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing parsed GTF data.\n",
    "    \"\"\"\n",
    "    print('Parsing GTF file....')\n",
    "    # Read the GTF file into a DataFrame\n",
    "    gtf_df = parse_gtf_to_dataframe(gtf_file)\n",
    "\n",
    "    # Convert gene names to lowercase for case-insensitive matching\n",
    "    if genes_str:\n",
    "            genes_of_interest = set([g.strip().lower() for g in genes_str.split(\",\")])\n",
    "            print(f\"Processing genes: {', '.join(genes_of_interest)}\")\n",
    "    else:\n",
    "        genes_of_interest = None\n",
    "        raise InputValueError(\"No gene list provided. Processing all genes. $genes_of_interest\", field=\"genes_of_interest\", code=\"no_gene_list_provided\")\n",
    "\n",
    "    # Check if a gene list is provided and filter accordingly\n",
    "    \n",
    "    if genes_of_interest:\n",
    "        if identifier_type == 'gene_id':\n",
    "            gtf_df = gtf_df[gtf_df['gene_id'].str.lower().isin(genes_of_interest) & ((gtf_df['feature'] == 'CDS') | (gtf_df['feature'] == 'exon'))]\n",
    "        elif identifier_type == 'gene_name':\n",
    "            gtf_df = gtf_df[gtf_df['gene_name'].str.lower().isin(genes_of_interest) & ((gtf_df['feature'] == 'CDS') | (gtf_df['feature'] == 'exon'))]\n",
    "\n",
    "        else:\n",
    "            raise InputValueError(\"Gene identifier type must be 'gene_id' or 'gene_name'\", field=\"identifier_type\", code=\"no_identifier_type_provided\")\n",
    "            \n",
    "\n",
    "    if len(gtf_df) == 0:\n",
    "        raise InputValueError(\"No matching genes found in the GTF file. This can be due to either incomplete gtf file or errors in gene identifications.\", field=\"genes_of_interest\", code=\"no_matching_genes_found\")\n",
    "    \n",
    "    return gtf_df, genes_of_interest\n",
    "\n",
    "\n",
    "def merge_regions_and_coverage(genes_of_interest, gtf_df):\n",
    "    \"\"\"\n",
    "    Merge CDS regions and computes average coverage for the specified genes.\n",
    "\n",
    "    Args:\n",
    "        genes_of_interest (set): A set of gene names to process.\n",
    "        gtf_df (pd.DataFrame): A DataFrame containing parsed GTF data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing merged CDS regions and average coverage.\n",
    "    \"\"\"\n",
    "    print('Merge regions and calculating coverage....')\n",
    "    # Initialize an empty list to store results\n",
    "    merged_regions = []\n",
    "\n",
    "    for gn in genes_of_interest:\n",
    "        # Subset the DataFrame for the given gene_name\n",
    "        isoforms = gtf_df[gtf_df['gene_name'].str.lower() == gn].copy()\n",
    "        \n",
    "        # Sort by chromosome, strand, and start position\n",
    "        isoforms = isoforms.sort_values(by=['seqname', 'strand', 'start'])\n",
    "\n",
    "        # Determine the full genomic range\n",
    "        min_start = isoforms['start'].min()\n",
    "        max_end = isoforms['end'].max()\n",
    "\n",
    "\n",
    "        # Create a NumPy array to track coverage over this genomic range\n",
    "        coverage_array = np.zeros(max_end - min_start + 1, dtype=int)\n",
    "\n",
    "        # Dictionary to track merged CDS regions\n",
    "        merged = []\n",
    "        \n",
    "        for _, row in isoforms.iterrows():\n",
    "            if not merged:\n",
    "                merged.append(row.to_dict())  \n",
    "            else:\n",
    "                prev = merged[-1]\n",
    "\n",
    "                if row['start'] <= prev['end']:  \n",
    "                    merged[-1]['end'] = max(prev['end'], row['end'])  \n",
    "                    merged[-1]['transcript_id'] += \";\" + row['transcript_id']  \n",
    "                else:\n",
    "                    merged.append(row.to_dict())  \n",
    "\n",
    "            # Update coverage using NumPy slicing (avoids looping over each position)\n",
    "            coverage_array[row['start'] - min_start : row['end'] - min_start + 1] += 1\n",
    "\n",
    "        # Convert merged results to a DataFrame\n",
    "        merged_df = pd.DataFrame(merged)\n",
    "\n",
    "        # Compute the average coverage for each merged region efficiently\n",
    "        coverage_values = []\n",
    "        for _, region in merged_df.iterrows():\n",
    "            region_slice = coverage_array[region['start'] - min_start : region['end'] - min_start + 1]\n",
    "            avg_coverage = np.mean(region_slice)  # Vectorized mean calculation\n",
    "            coverage_values.append(avg_coverage)\n",
    "\n",
    "        # Add coverage column to DataFrame\n",
    "        merged_df['coverage'] = coverage_values\n",
    "\n",
    "        merged_regions.append(merged_df)\n",
    "\n",
    "    # Concatenate all gene-specific DataFrames into a final result\n",
    "    final_df = pd.concat(merged_regions, ignore_index=True)\n",
    "\n",
    "    # Add region_id column for easier downstream analysis\n",
    "    final_df['region'] = final_df['seqname'].astype(str) + \":\" + final_df['start'].astype(str) + \"-\" + final_df['end'].astype(str)\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    final_df = final_df[['seqname', 'start', 'end', 'strand', 'gene_name', 'transcript_id', 'coverage', 'region']]\n",
    "\n",
    "    # Display the DataFrame with coverage\n",
    "    return(final_df)\n",
    "\n",
    "## Extract sequences functions:\n",
    "def save_regions_for_faidx(df, output_file, plp_length=30, identifier_type = 'gene_name'):\n",
    "    \"\"\"\n",
    "    Saves genomic regions in a format compatible with `samtools faidx`.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'seqname', 'start', 'end' columns.\n",
    "        output_file (str): Path to save the region file.\n",
    "    \"\"\"\n",
    "    print(f\"Saving regions to {output_file} for `samtools faidx`...\")\n",
    "\n",
    "    # Setting the datatype\n",
    "    df['seqname'] = df['seqname'].astype(str)\n",
    "    df['start'] = df['start'].astype(int)  \n",
    "    df['end'] = df['end'].astype(int) \n",
    "    plp_length = int(plp_length)\n",
    "\n",
    "\n",
    "    # Filter regions based on length\n",
    "    df = df[(df['end'] - df['start']) >= (plp_length + plp_length / 2)]\n",
    "    \n",
    "    # Export regions in the format expected by `samtools faidx`\n",
    "    df[['region']].to_csv(output_file + \".txt\", sep = '\\t', header=False, index=False)\n",
    "\n",
    "def check_fasta_index(fasta_file):\n",
    "    \"\"\"\n",
    "    Checks if a FASTA index file (.fai) exists.\n",
    "\n",
    "    Args:\n",
    "        fasta_file (str): Path to the FASTA file.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the valid FASTA index file.\n",
    "    \"\"\"\n",
    "    fai_file_1 = fasta_file + \".fai\"  # Example: reference.fa.fai\n",
    "    fai_file_2 = os.path.splitext(fasta_file)[0] + \".fai\"  # Example: reference.fai\n",
    "\n",
    "    if os.path.exists(fai_file_1):\n",
    "        return fai_file_1\n",
    "    elif os.path.exists(fai_file_2):\n",
    "        return fai_file_2\n",
    "    else:\n",
    "        print(f\"⚠️ FASTA index file not found for {fasta_file}!\")\n",
    "        print(\"Creating the index now... This may take a while for large genomes.\")\n",
    "        subprocess.run([\"samtools\", \"faidx\", fasta_file])\n",
    "        return fasta_file + \".fai\"  # Assume samtools follows the convention\n",
    "\n",
    "\n",
    "def extract_sequences(fasta_file, regions_file, output_fasta, gtf_df):\n",
    "    \"\"\"\n",
    "    Extracts CDS sequences using `samtools faidx`, adjusts for strand orientation, \n",
    "    and includes gene identifiers in the FASTA headers.\n",
    "\n",
    "    Args:\n",
    "        fasta_file (str): Path to the indexed FASTA file.\n",
    "        regions_file (str): File with genomic regions (one per line).\n",
    "        output_fasta (str): Path to save the extracted sequences.\n",
    "        gtf_df (pd.DataFrame): DataFrame containing 'seqname', 'start', 'end', 'strand', 'gene_name' for strand correction.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting sequences from {fasta_file} using `samtools faidx`...\")\n",
    "\n",
    "    # Get number of CPUs for multi-threading\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "    # Temporary output file before strand correction\n",
    "    temp_fasta = \"temp_extracted.fa\"\n",
    "\n",
    "    # Run samtools faidx to extract sequences\n",
    "    command = [\n",
    "        \"samtools\", \"faidx\", \"-@\", str(num_cpus), \n",
    "        \"-r\", regions_file, \n",
    "        \"-o\", temp_fasta, \n",
    "        fasta_file\n",
    "    ]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "    print(\"✅ Sequences extracted. Now adjusting for strand orientation and updating headers...\")\n",
    "\n",
    "    # Load region-to-gene mapping from gtf_df\n",
    "    feature_dict = dict(zip(\n",
    "        gtf_df['region'],\n",
    "        zip(gtf_df['gene_name'], gtf_df['strand'])  \n",
    "    ))\n",
    "\n",
    "    # Read extracted sequences and apply strand correction\n",
    "    updated_sequences = []\n",
    "    seq_dict = SeqIO.to_dict(SeqIO.parse(temp_fasta, \"fasta\"))  \n",
    "\n",
    "    for region, record in seq_dict.items():\n",
    "        gene_name, strand = feature_dict.get(region, (\"UNKNOWN\", \"+\"))  \n",
    "        sequence = record.seq\n",
    "\n",
    "        # Reverse-complement if the gene is on the negative strand\n",
    "        if strand == \"-\":\n",
    "            sequence = sequence.reverse_complement()\n",
    "\n",
    "        # Update header: >gene_name|region\n",
    "        record.id = f\"{gene_name}|{region}\"\n",
    "        record.description = \"\"  # Remove extra description\n",
    "        record.seq = sequence\n",
    "        updated_sequences.append(record)\n",
    "\n",
    "    # Write the updated FASTA\n",
    "    SeqIO.write(updated_sequences, output_fasta, \"fasta\")\n",
    "\n",
    "    print(f\"✅ Final sequences saved to {output_fasta}, with correct strand orientation and gene names.\")\n",
    "\n",
    "## Extract mrna function:\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Data import CodonTable\n",
    "\n",
    "def parse_attributes(attr_str):\n",
    "    \"\"\"\n",
    "    Parse the attributes column from a GTF file.\n",
    "    Example: 'transcript_id \"TX1\"; gene_id \"G1\"; ...'\n",
    "    Returns a dictionary mapping keys to values.\n",
    "    \"\"\"\n",
    "    attrs = {}\n",
    "    for part in attr_str.strip().split(';'):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        m = re.match(r'(\\S+)\\s+\"(.+)\"', part)\n",
    "        if m:\n",
    "            key, value = m.groups()\n",
    "            attrs[key] = value\n",
    "        else:\n",
    "            pieces = part.split()\n",
    "            if len(pieces) >= 2:\n",
    "                key = pieces[0]\n",
    "                value = pieces[1].strip('\"')\n",
    "                attrs[key] = value\n",
    "    return attrs\n",
    "\n",
    "def extract_mrna_sequences(fasta_file, gtf_file, output_file=None,\n",
    "                           plus_strand_only=False, revcomp=False,\n",
    "                           translate=False, codon_table=1,\n",
    "                           alternative_start_codon=False,\n",
    "                           clean_final_stop=False, clean_internal_stop=False,\n",
    "                           verbose=False):\n",
    "    \"\"\"\n",
    "    Extracts mRNA sequences from a FASTA file using exon records from a GTF file.\n",
    "    \n",
    "    The function assumes that the GTF file uses 1-indexed, inclusive coordinates.\n",
    "    Each exon is extracted as: [start-1:end] (Python slicing).\n",
    "    \n",
    "    For both plus and negative strands, exons are first sorted in ascending order.\n",
    "    For negative strand transcripts the merged sequence is then reverse complemented\n",
    "    so that the output is in the 5'->3' orientation.\n",
    "    \n",
    "    Upstream/downstream extractions have been removed.\n",
    "    \n",
    "    Parameters:\n",
    "      fasta_file (str): Path to the reference FASTA file.\n",
    "      gtf_file (str): Path to the GTF file with exon annotations.\n",
    "      output_file (str, optional): If provided, the output FASTA will be written here.\n",
    "      plus_strand_only (bool): If True, output sequence in plus strand orientation.\n",
    "      revcomp (bool): If True, force reverse complement of the final sequence.\n",
    "      translate (bool): If True, translate the nucleotide sequence.\n",
    "      codon_table (int): NCBI codon table ID (default 1).\n",
    "      alternative_start_codon (bool): If True, force a methionine (M) at the start if valid.\n",
    "      clean_final_stop (bool): If True, remove a trailing stop codon after translation.\n",
    "      clean_internal_stop (bool): If True, replace internal stop codons with 'X'.\n",
    "      verbose (bool): If True, print progress messages.\n",
    "      \n",
    "    Returns:\n",
    "      List of SeqRecord objects (one per transcript).\n",
    "    \"\"\"\n",
    "    # Index the genome FASTA for quick lookup.\n",
    "    genome = SeqIO.to_dict(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "    \n",
    "    # Add error handling for missing FASTA and gtf entries.\n",
    "    if not genome:\n",
    "        raise InputValueError(f\"FASTA file not found: {fasta_file}\", field=\"fasta_file\", code=\"fasta_file_not_found\")\n",
    "    if not os.path.isfile(gtf_file):\n",
    "        raise InputValueError(f\"GTF file not found: {gtf_file}\", field=\"gtf_file\", code=\"gtf_file_not_found\")\n",
    "    \n",
    "\n",
    "    # Group exon features by transcript_id.\n",
    "    transcripts = {}  # transcript_id -> {'chrom': ..., 'strand': ..., 'gene_id': ..., 'exons': [(start, end), ...]}\n",
    "    with open(gtf_file, \"r\") as gtf:\n",
    "        for line in gtf:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            fields = line.strip().split(\"\\t\")\n",
    "            if len(fields) < 9:\n",
    "                continue\n",
    "            chrom, source, feature, start, end, score, strand, frame, attributes = fields\n",
    "            if feature.lower() != \"exon\":\n",
    "                continue\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            attr_dict = parse_attributes(attributes)\n",
    "            transcript_id = attr_dict.get(\"transcript_id\")\n",
    "            gene_id = attr_dict.get(\"gene_id\", \"\")\n",
    "            gene_name = attr_dict.get(\"gene_name\", \"\")\n",
    "            if transcript_id is None:\n",
    "                continue\n",
    "            if transcript_id not in transcripts:\n",
    "                transcripts[transcript_id] = {\"chrom\": chrom, \"strand\": strand, \"gene_id\": gene_id, \"exons\": [], \"gene_name\": gene_name}\n",
    "            transcripts[transcript_id][\"exons\"].append((start, end))\n",
    "    \n",
    "    records = []\n",
    "    for transcript_id, info in transcripts.items():\n",
    "        chrom = info[\"chrom\"]\n",
    "        strand = info[\"strand\"]\n",
    "        gene_id = info[\"gene_id\"]\n",
    "        exons = info[\"exons\"]\n",
    "        gene_name = info[\"gene_name\"]\n",
    "        if chrom not in genome:\n",
    "            if verbose:\n",
    "                print(f\"Warning: Chromosome {chrom} not found in FASTA for transcript {transcript_id}.\")\n",
    "            continue\n",
    "        chrom_seq = genome[chrom].seq\n",
    "\n",
    "        # Always sort exons in ascending order.\n",
    "        exons_sorted = sorted(exons, key=lambda x: x[0])\n",
    "        # Extract each exon using 1-indexed, inclusive conversion:\n",
    "        # Python slice [s-1:e] returns bases s to e (inclusive).\n",
    "        exon_seqs = [chrom_seq[s-1:e] for s, e in exons_sorted]\n",
    "        merged_seq = Seq(\"\").join(exon_seqs)\n",
    "        \n",
    "        # For negative strand, reverse complement the merged sequence (unless forced to plus strand).\n",
    "        if strand == \"-\" and not plus_strand_only:\n",
    "            merged_seq = merged_seq.reverse_complement()\n",
    "        # Additionally, if revcomp is set, always reverse complement.\n",
    "        if revcomp:\n",
    "            merged_seq = merged_seq.reverse_complement()\n",
    "        \n",
    "        final_seq = merged_seq\n",
    "        \n",
    "        record_description = f\"gene={gene_id} gene_name={gene_name} seq_id={chrom} type=mrna\"\n",
    "        \n",
    "        # Optional translation.\n",
    "        if translate:\n",
    "            prot_seq = final_seq.translate(table=codon_table, to_stop=False)\n",
    "            if alternative_start_codon:\n",
    "                table_obj = CodonTable.unambiguous_dna_by_id[codon_table]\n",
    "                start_codon = str(final_seq[0:3])\n",
    "                if start_codon in table_obj.start_codons and prot_seq[0] != \"M\":\n",
    "                    prot_seq = \"M\" + str(prot_seq)[1:]\n",
    "            if clean_final_stop and str(prot_seq).endswith(\"*\"):\n",
    "                prot_seq = prot_seq[:-1]\n",
    "            if clean_internal_stop:\n",
    "                if str(prot_seq).endswith(\"*\"):\n",
    "                    prot_seq = prot_seq[:-1].replace(\"*\", \"X\") + \"*\"\n",
    "                else:\n",
    "                    prot_seq = str(prot_seq).replace(\"*\", \"X\")\n",
    "                prot_seq = Seq(prot_seq)\n",
    "            final_seq = prot_seq\n",
    "            record_description += \" translated\"\n",
    "        \n",
    "        record = SeqRecord(final_seq, id=transcript_id, description=record_description)\n",
    "        records.append(record)\n",
    "    \n",
    "    if output_file:\n",
    "        with open(output_file, \"w\") as out_handle:\n",
    "            SeqIO.write(records, out_handle, \"fasta\")\n",
    "        if verbose:\n",
    "            print(f\"Wrote {len(records)} transcripts to {output_file}\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "## Find target functions:\n",
    "def find_targets_deprecated(selected_features, fasta_file, plp_length, min_coverage, output_file='Candidate_probes.txt', gc_min=50, gc_max=65, num_probes=10, iupac_mismatches=None):\n",
    "    \"\"\"\n",
    "    Function to extract target sequences fulfilling the following criteria:\n",
    "    Adapted from sequence developed by Sergio \n",
    "    Args:\n",
    "        selected_features (str): Path to the selected features file (TSV format).\n",
    "        fasta_file (str): Path to the indexed FASTA file.\n",
    "        output_file (str): Path to the output file.\n",
    "        plp_length (int): Probe length (default: 30).\n",
    "        min_coverage (int): Minimum coverage of the region.\n",
    "        gc_min (int): Minimum GC content (default: 50).\n",
    "        gc_max (int): Maximum GC content (default: 65).\n",
    "        num_probes (int): Number of probes to select per gene (default: 10).\n",
    "        iupac_mismatches (position:base): \n",
    "            List of positions (1-based) and IUPAC codes to introduce mismatches.        \n",
    "            Recommended positions for mismatches are **15 and 16**.\n",
    "            Example: 15:R,16:G\n",
    "            **Note:** The total number of mismatches must be ≤2. Exceeding this limit may lead to unexpected behavior.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with extracted probe sequences.\n",
    "    \"\"\"\n",
    "    # Create a dataframe to store the targets\n",
    "    targets = pd.DataFrame(columns=['Probe_id', 'Gene', 'Region', 'Sequence', 'GC', 'Coverage', 'Transcript_id'])\n",
    "\n",
    "\n",
    "    # check if size of the probe is an even number\n",
    "    if plp_length % 2 != 0:\n",
    "        raise InputValueError(\"The size of the probe should be an even number\", field=\"plp_length\", code=\"odd_value_for_plp_length_provided\")\n",
    "    \n",
    "        \n",
    "    # load the fasta file\n",
    "    seq_dict = SeqIO.to_dict(SeqIO.parse(fasta_file, \"fasta\"))  \n",
    "\n",
    "    # load the selected features table with gene name and region\n",
    "    selected_features= pd.read_csv(selected_features, sep='\\t')\n",
    "    selected_features.index = selected_features['region']\n",
    "    \n",
    "    # Convert selected_features into a dictionary for fast lookup\n",
    "    coverage_dict = dict(zip(selected_features['region'], selected_features['coverage']))\n",
    "\n",
    "\n",
    "    # Initialize DataFrame for results\n",
    "    targets = []\n",
    "\n",
    "    # loop through the fasta file\n",
    "    for keys in seq_dict:\n",
    "        # extract gene and region of the sequence\n",
    "        gene, region = keys.split(\"|\")\n",
    "\n",
    "        # Extract chromosome and start position from region\n",
    "        chr_name, start_end = region.split(\":\")\n",
    "        initial_start = int(start_end.split(\"-\")[0])\n",
    "\n",
    "        # Extract the sequence\n",
    "        seq = seq_dict[keys].seq\n",
    "        seq_len = len(seq)-(plp_length-1) # Limit the sequence length to extract probes\n",
    "\n",
    "        # Keep track of the coverage value for the region and check the coverage to be greater than the minimum coverage\n",
    "        coverage_value = coverage_dict.get(region, 0)\n",
    "        if coverage_value < min_coverage:\n",
    "            continue \n",
    "\n",
    "        # Loop through the sequence\n",
    "        for i in range(0, seq_len):\n",
    "            # Extract the probe sequence\n",
    "            tmp_seq = seq[i:i+plp_length]\n",
    "            # Calculate the gc content and check if it is within the range\n",
    "            gc_content = gc_fraction(tmp_seq)*100\n",
    "\n",
    "            if gc_content < gc_min or gc_content > gc_max:\n",
    "                continue\n",
    "\n",
    "            if not any(nucleotide * 3 in tmp_seq for nucleotide in \"ACGT\"):\n",
    "                continue\n",
    "\n",
    "            start = initial_start + i\n",
    "            end = start + plp_length -1\n",
    "\n",
    "            # save the target to the dataframe\n",
    "            targets.append({\n",
    "                \"Probe_id\": f\"{gene}|{start}-{end}\",\n",
    "                \"Gene\": gene,\n",
    "                \"Region\": f\"{chr_name}:{start}-{end}\",\n",
    "                \"Sequence\": str(tmp_seq),\n",
    "                \"GC\": gc_content,\n",
    "                \"Coverage\": coverage_value,\n",
    "                \"Transcript_id\": selected_features.loc[region, 'transcript_id']\n",
    "            })\n",
    "    targets_df = pd.DataFrame(targets)\n",
    "    targets_df = evaluate_ligation_junction(targets_df, iupac_mismatches=iupac_mismatches, plp_length=plp_length)\n",
    "\n",
    "    # Remove non-preferred ligation junctions\n",
    "    targets_df = select_top_probes(targets_df, num_probes)\n",
    "    create_fasta(targets_df, output_file)\n",
    "    \n",
    "    print(f\"✅ Final sequences saved to {output_file} and fasta file {output_file}.fa .\")\n",
    "    return targets_df\n",
    "\n",
    "def create_fasta(targets_df, output_file):\n",
    "    \"\"\"\n",
    "    Create a FASTA file from the DataFrame with probe sequences.\n",
    "\n",
    "    Args:\n",
    "        targets_df (DataFrame): DataFrame with probe sequences.\n",
    "        output_file (str): Path to the output file.\n",
    "    \"\"\"\n",
    "    output_file = output_file + \".fa\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _,row in targets_df.iterrows():\n",
    "            f.write(f\">{row['Probe_id']}\\n{row['Sequence']}\\n\")\n",
    "\n",
    "def select_top_probes(df, num_probes):\n",
    "    \"\"\"\n",
    "    Select the top probes based on coverage and ligation junction preferences.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame with probe sequences.\n",
    "        num_probes (int): Number of probes to select.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with the top probes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the dataframe: Highest coverage first, then ligation junction order\n",
    "    df_sorted = df.sort_values(by=[\"Coverage\", \"Ligation junction\"], \n",
    "                               ascending=[False, True], \n",
    "                               key=lambda col: col.map({'preferred': 0, 'neutral': 1, 'non-preferred': 2}).fillna(3))\n",
    "    # Remove probes non-preferred ligation junctions\n",
    "    df_sorted = df_sorted[df_sorted['Ligation junction'] != 'non-preferred']\n",
    "    \n",
    "    # Select top `num_probes`\n",
    "    final_df = df_sorted.groupby(\"Gene\").head(num_probes)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "## Check specificity functions:\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import dnaio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from cutadapt.adapters import BackAdapter\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "@dataclass\n",
    "class Alignment:\n",
    "    query_name: str\n",
    "    target_name: str\n",
    "    target_start: int\n",
    "    target_end: int\n",
    "    mismatches: int\n",
    "    query_sequence: str\n",
    "    target_sequence: str\n",
    "\n",
    "def find_probes_in_targets(targets_df, reference_fasta, max_errors=1, output_file=None):\n",
    "    \"\"\"\n",
    "    Function to check specificity of extracted probes against a reference genome.\n",
    "    Uses Cutadapt's aligner for finding target regions.\n",
    "\n",
    "    Args:\n",
    "        targets_df (DataFrame): DataFrame containing extracted probe sequences.\n",
    "        reference_fasta (str): Path to the reference genome FASTA.\n",
    "        max_errors (int): Maximum number of allowed mismatches.\n",
    "        output_file (str, optional): Path to save the results as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing matched probe alignments.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    with dnaio.open(reference_fasta) as references:\n",
    "        references = list(references)  # Load references into a list\n",
    "        total_probes = len(targets_df)  # Get total probe count\n",
    "\n",
    "        with tqdm(total=total_probes, desc=\"Testing probes for specificity\", unit=\" alignments\") as pbar:\n",
    "            for reference_record in references:\n",
    "                ref_id = reference_record.id\n",
    "                ref_seq = reference_record.sequence\n",
    "\n",
    "                for _, row in targets_df.iterrows():\n",
    "                    probe_id = row[\"Probe_id\"]\n",
    "                    probe_seq = row[\"Sequence\"]\n",
    "\n",
    "                    adapter = BackAdapter(probe_seq, max_errors=max_errors, min_overlap=len(probe_seq), indels=False)\n",
    "                    aligner = adapter.aligner\n",
    "\n",
    "                    # Forward strand search\n",
    "                    for t_start, t_end, errors, target_seq in find_all(ref_seq, aligner):\n",
    "                        results.append(Alignment(\n",
    "                            query_name=probe_id,\n",
    "                            target_name=ref_id,\n",
    "                            target_start=t_start + 1,  # Convert to 1-based index\n",
    "                            target_end=t_end,\n",
    "                            mismatches=errors,\n",
    "                            query_sequence=probe_seq,\n",
    "                            target_sequence=target_seq\n",
    "                        ))\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "\n",
    "                    # Reverse complement search\n",
    "                    rev_ref_seq = str(Seq(ref_seq).reverse_complement())\n",
    "                    adapter = BackAdapter(probe_seq, max_errors=max_errors, min_overlap=len(probe_seq), indels=False)\n",
    "                    aligner = adapter.aligner\n",
    "\n",
    "                    for t_start, t_end, errors, target_seq in find_all(rev_ref_seq, aligner):\n",
    "                        results.append(Alignment(\n",
    "                            query_name=probe_id,\n",
    "                            target_name=f\"{ref_id}(reverse)\",\n",
    "                            target_start=t_start + 1,\n",
    "                            target_end=t_end,\n",
    "                            mismatches=errors,\n",
    "                            query_sequence=probe_seq,\n",
    "                            target_sequence=target_seq\n",
    "                        ))\n",
    "                    pbar.update(1)  # Update progress bar again for reverse search\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results to file if requested\n",
    "    if output_file:\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        sys.stderr.write(f\"Results saved to {output_file}\\n\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def find_all(ref, aligner):\n",
    "    \"\"\"Find all occurrences of a probe in a reference sequence.\"\"\"\n",
    "    offset = 0\n",
    "    while True:\n",
    "        result = aligner.locate(ref)\n",
    "        if result is None:\n",
    "            break\n",
    "        ref_start, ref_end, query_start, query_end, score, errors = result\n",
    "        t_start = query_start + offset\n",
    "        t_end = query_end + offset\n",
    "        target_seq = ref[query_start:query_end]\n",
    "        yield (t_start, t_end, errors, target_seq)\n",
    "        offset += query_start + 1\n",
    "        ref = ref[query_start + 1:]\n",
    "\n",
    "def find_targets(selected_features, fasta_file, reference_fasta, plp_length=30, min_coverage=1,\n",
    "                 output_file='Candidate_probes', gc_min=50, gc_max=65, num_probes=10,\n",
    "                 iupac_mismatches=None, max_errors=1, check_specificity=False, off_target_output=False):\n",
    "    \"\"\"\n",
    "    Extract target sequences based on defined probe criteria and optionally check specificity.\n",
    "\n",
    "    Args:\n",
    "        selected_features (str): Path to the selected features file (TSV format).\n",
    "        fasta_file (str): Path to the indexed FASTA file.\n",
    "        reference_fasta (str): Path to the reference genome FASTA.\n",
    "        output_file (str): Base path for the output files.\n",
    "        plp_length (int): Probe length (default: 30).\n",
    "        min_coverage (int): Minimum coverage of the region.\n",
    "        gc_min (int): Minimum GC content (default: 50).\n",
    "        gc_max (int): Maximum GC content (default: 65).\n",
    "        num_probes (int): Number of probes to select per gene (default: 10).\n",
    "        iupac_mismatches (str): IUPAC mismatch specification (e.g., \"15:R,16:G\").\n",
    "        max_errors (int): Maximum mismatches allowed during specificity checking.\n",
    "        check_specificity (bool): Whether to check probe specificity against reference.\n",
    "        off_target_output (bool): Whether to save off-target probe information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame with extracted (and filtered) probe sequences, off-target details or None)\n",
    "    \"\"\"\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    # Read the FASTA file and selected features\n",
    "    seq_dict = SeqIO.to_dict(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "    selected_features = pd.read_csv(selected_features, sep='\\t')\n",
    "    selected_features.index = selected_features['region']\n",
    "    coverage_dict = dict(zip(selected_features['region'], selected_features['coverage']))\n",
    "\n",
    "    if plp_length % 2 != 0:\n",
    "        raise InputValueError(\"The size of the probe should be an even number\", field=\"plp_length\", code=\"odd_value_for_plp_length_provided\")\n",
    "\n",
    "    for key in seq_dict:\n",
    "        gene, region = key.split(\"|\")\n",
    "        chr_name, start_end = region.split(\":\")\n",
    "        initial_start = int(start_end.split(\"-\")[0])\n",
    "        seq = seq_dict[key].seq\n",
    "        seq_len = len(seq) - (plp_length - 1)\n",
    "        coverage_value = coverage_dict.get(region, 0)\n",
    "        if coverage_value < min_coverage:\n",
    "            continue \n",
    "\n",
    "        for i in range(seq_len):\n",
    "            tmp_seq = seq[i:i + plp_length]\n",
    "            gc_content = (tmp_seq.count(\"G\") + tmp_seq.count(\"C\")) / len(tmp_seq) * 100\n",
    "\n",
    "            if not (gc_min <= gc_content <= gc_max):\n",
    "                continue\n",
    "            if not any(nucleotide * 3 in tmp_seq for nucleotide in \"ACGT\"):\n",
    "                continue\n",
    "\n",
    "            start = initial_start + i\n",
    "            end = start + plp_length - 1\n",
    "\n",
    "            targets.append({\n",
    "                \"Probe_id\": f\"{gene}|{chr_name}:{start}-{end}\",\n",
    "                \"Gene\": gene,\n",
    "                \"Region\": f\"{chr_name}:{start}-{end}\",\n",
    "                \"Sequence\": str(tmp_seq),\n",
    "                \"GC\": gc_content,\n",
    "                \"Coverage\": coverage_value,\n",
    "                \"Transcript_id\": selected_features.loc[region, 'transcript_id']\n",
    "            })\n",
    "\n",
    "    targets_df = pd.DataFrame(targets)\n",
    "\n",
    "    # Introduce IUPAC mismatches if specified and check ligation junctions. Or just check ligation junctions\n",
    "\n",
    "    targets_df = evaluate_ligation_junction(targets_df, iupac_mismatches=iupac_mismatches, plp_length=plp_length)\n",
    "\n",
    "    off_target_info = None  # default when specificity is not checked\n",
    "\n",
    "    # Check probe specificity against reference genome if requested\n",
    "    if check_specificity:\n",
    "        specificity_results = find_probes_in_targets(targets_df, reference_fasta, max_errors, \n",
    "                                                     output_file=f\"{output_file}_specificity.csv\")\n",
    "        off_target_file = f\"{output_file}_off_targets.csv\" if off_target_output else None\n",
    "        valid_targets_df, off_target_info = filter_probes_by_specificity(targets_df, specificity_results,\n",
    "                                                                         off_target_output_file=off_target_file)\n",
    "        targets_df = valid_targets_df\n",
    "        print(f\"✅ Specificity results saved to {output_file}_specificity.csv\")\n",
    "\n",
    "    # Further filtering (e.g., select_top_probes) can be done here if desired\n",
    "\n",
    "    return targets_df, off_target_info\n",
    "\n",
    "\n",
    "def filter_probes_by_specificity(targets_df, specificity_results, off_target_output_file=None):\n",
    "    \"\"\"\n",
    "    Filter candidate probes by verifying that all observed specificity results match the expected transcript IDs.\n",
    "    \n",
    "    Args:\n",
    "        targets_df (DataFrame): DataFrame with candidate probes. Expected to have:\n",
    "            - 'Probe_id'\n",
    "            - 'Transcript_id' (semicolon-separated string)\n",
    "        specificity_results (DataFrame): DataFrame with specificity results. Expected to have:\n",
    "            - 'query_name'\n",
    "            - 'target_name'\n",
    "        off_target_output_file (str, optional): Path to save off-target probe information.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame of valid probes reassembled to one row per probe, list of off-target details)\n",
    "    \"\"\"\n",
    "    # Expand targets_df by splitting Transcript_id on ';'\n",
    "    targets_expanded = targets_df.assign(\n",
    "        Transcript_id=targets_df['Transcript_id'].str.split(';')\n",
    "    ).explode('Transcript_id')\n",
    "    \n",
    "    # Create composite key for expected mappings\n",
    "    targets_expanded['probe_id_transcript_id'] = targets_expanded['Probe_id'] + '_' + targets_expanded['Transcript_id']\n",
    "    \n",
    "    # Create composite key in specificity results\n",
    "    specificity_results['probe_id_transcript_id'] = specificity_results['query_name'] + '_' + specificity_results['target_name']\n",
    "    \n",
    "    valid_probes = []\n",
    "    off_target_details = []  # To capture details of off-target probes\n",
    "    \n",
    "    # Evaluate each probe's specificity\n",
    "    for probe_id, group in targets_expanded.groupby('Probe_id'):\n",
    "        expected = set(group['probe_id_transcript_id'])\n",
    "        observed = set(specificity_results.loc[specificity_results['query_name'] == probe_id, 'probe_id_transcript_id'])\n",
    "        \n",
    "        if observed.issubset(expected):\n",
    "            valid_probes.append(probe_id)\n",
    "        else:\n",
    "            off_target_details.append({\n",
    "                'Probe_id': probe_id,\n",
    "                'Expected': expected,\n",
    "                'Observed': observed,\n",
    "                'Off_targets': observed - expected\n",
    "            })\n",
    "    \n",
    "    # Filter to keep only valid probes\n",
    "    valid_targets = targets_expanded[targets_expanded['Probe_id'].isin(valid_probes)]\n",
    "    \n",
    "    # Reassemble transcript IDs into a semicolon-separated string per probe\n",
    "    valid_targets_df = valid_targets.groupby(\n",
    "        ['Probe_id', 'Gene', 'Region', 'Sequence', 'GC', 'Coverage'], as_index=False\n",
    "    ).agg({'Transcript_id': ';'.join})\n",
    "    \n",
    "    # Optionally save off-target details to a CSV file\n",
    "    if off_target_output_file and off_target_details:\n",
    "        off_target_df = pd.DataFrame(off_target_details)\n",
    "        off_target_df.to_csv(off_target_output_file, index=False)\n",
    "        print(f\"Off-target probe details saved to {off_target_output_file}\")\n",
    "    \n",
    "    return valid_targets_df, off_target_df\n",
    "\n",
    "def parse_specificity_results(specificity_results):\n",
    "    \"\"\"\n",
    "    Parse the specificity results from the CSV file.\n",
    "\n",
    "    Args:\n",
    "        specificity_results (str): Path to the specificity results CSV file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing parsed specificity results.\n",
    "    \"\"\"\n",
    "    specificity_results.group_by('Probe_id').size().reset_index(name='counts')\n",
    "    return pd.read_csv(specificity_results)\n",
    "\n",
    "def filter_probes_by_distance(group, min_dist_probes):\n",
    "    \"\"\"\n",
    "    Filters probes based on a minimum distance between them.\n",
    "    This function takes a DataFrame containing probe information and filters out probes\n",
    "    that are too close to each other based on a specified minimum distance. The 'Region'\n",
    "    column in the DataFrame should contain coordinates in the format 'chr:start-end'.\n",
    "    Args:\n",
    "        group (pd.DataFrame): A DataFrame containing probe information with a 'Region' column.\n",
    "        min_dist_probes (int): The minimum distance required between probes.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the filtered probes that meet the minimum distance requirement.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract start and end coordinates from the 'Region' column\n",
    "    group['Start'] = group['Region'].apply(lambda x: int(x.split(':')[1].split('-')[0]))\n",
    "    group['End'] = group['Region'].apply(lambda x: int(x.split(':')[1].split('-')[1]))\n",
    "    \n",
    "    # Sort by start coordinate\n",
    "    group = group.sort_values(by='Start')\n",
    "    \n",
    "    # Filter probes based on the minimum distance\n",
    "    filtered_probes = []\n",
    "    last_end = -min_dist_probes  # Initialize with a value that ensures the first probe is included\n",
    "    \n",
    "    for idx, row in group.iterrows():\n",
    "        if row['Start'] - last_end >= min_dist_probes:\n",
    "            filtered_probes.append(row)\n",
    "            last_end = row['End']\n",
    "    filtered_probes = pd.DataFrame(filtered_probes)        \n",
    "    filtered_probes = filtered_probes.drop(columns=['Start', 'End'])\n",
    "    return pd.DataFrame(filtered_probes)\n",
    "\n",
    "\n",
    "## Calculating weighted melting temperature functions:\n",
    "def weighted_tm_gc_scoring(sequence, Tm_oligo, Tm_min=55.0, Tm_opt=60.0, Tm_max=65.0, \n",
    "                           GC_min=40.0, GC_opt=50.0, GC_max=60.0, w_Tm=1.0, w_GC=1.0):\n",
    "    \"\"\"Compute the weighted score for a given oligo based on melting temperature and GC content.\"\"\"\n",
    "    GC_oligo = gc_fraction(sequence) * 100  # Convert fraction to percentage\n",
    "    Tm_opt = (Tm_max + Tm_min) / 2\n",
    "    # Compute Tm deviation score\n",
    "    if Tm_oligo >= Tm_opt:\n",
    "        score_Tm = abs(Tm_oligo - Tm_opt) / (Tm_max - Tm_opt)\n",
    "    else:\n",
    "        score_Tm = abs(Tm_oligo - Tm_opt) / (Tm_opt - Tm_min)\n",
    "    \n",
    "    # Compute GC deviation score\n",
    "    if GC_oligo >= GC_opt:\n",
    "        score_GC = abs(GC_oligo - GC_opt) / (GC_max - GC_opt)\n",
    "    else:\n",
    "        score_GC = abs(GC_oligo - GC_opt) / (GC_opt - GC_min)\n",
    "    \n",
    "    return w_Tm * score_Tm + w_GC * score_GC  # Lower score is better\n",
    "\n",
    "\n",
    "def analyze_scores(scores, percentile=5):\n",
    "    \"\"\"Analyze the score distribution and suggest a cutoff.\"\"\"\n",
    "    suggested_cutoff = np.percentile(scores, percentile)  # Get the threshold for top X%\n",
    "    return suggested_cutoff\n",
    "\n",
    "\n",
    "def score_padlock_probe(sequence, Tm_min=55.0, Tm_opt=60.0, Tm_max=65.0, \n",
    "                        GC_min=40.0, GC_opt=50.0, GC_max=60.0, w_Tm=1.0, w_GC=1.0, percentile=5):\n",
    "    \"\"\"Score a padlock probe by splitting it into two arms and analyzing distribution.\"\"\"\n",
    "    mid = len(sequence) // 2\n",
    "    left_arm, right_arm = sequence[:mid], sequence[mid:]\n",
    "\n",
    "    # Compute Tm for each arm\n",
    "    Tm_left = mt.Tm_NN(left_arm)\n",
    "    Tm_right = mt.Tm_NN(right_arm)\n",
    "\n",
    "    # Score each arm separately\n",
    "    score_left = weighted_tm_gc_scoring(left_arm, Tm_left, Tm_min, Tm_opt, Tm_max, GC_min, GC_opt, GC_max, w_Tm, w_GC)\n",
    "    score_right = weighted_tm_gc_scoring(right_arm, Tm_right, Tm_min, Tm_opt, Tm_max, GC_min, GC_opt, GC_max, w_Tm, w_GC)\n",
    "\n",
    "    # Combine scores (you can take the average, max, or another approach)\n",
    "    final_score = (score_left + score_right) / 2  # Averaging the scores\n",
    "\n",
    "    return final_score\n",
    "def visualize_score_distribution(scores, cutoff):\n",
    "    \"\"\"Plot the score distribution and cutoff.\"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(scores, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.axvline(cutoff, color='red', linestyle='dashed', linewidth=2, label=f'Cutoff ({cutoff:.2f})')\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Probe Scores\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 codes/run_probe_design.py \\\n",
    "--extract_features_gtf data/Mus_musculus.GRCm39.113.chr.gtf \\\n",
    "--extract_features_genes Acta2,Cbs,Gfap,Hapln2,Plp1 \\\n",
    "--extract_features_identifier_type gene_name \\\n",
    "--extract_features_gene_feature CDS \\\n",
    "--extract_features_output extract_features_output.txt \\\n",
    "--extract_transcriptome_gtf data/Mus_musculus.GRCm39.113.chr.gtf \\\n",
    "--extract_transcriptome_fasta data/Mus_musculus.GRCm39.dna.primary_assembly.fa \\\n",
    "--extract_transcriptome_output_file data/transcriptome_out.fa \\\n",
    "--extract_sequences_fasta data/Mus_musculus.GRCm39.dna.primary_assembly.fa \\\n",
    "--extract_sequences_output_fasta extract_seqs_output.fa \\\n",
    "--extract_sequences_identifier_type gene_name \\\n",
    "--extract_sequences_plp_length 30 \\\n",
    "--extract_sequences_gtf_output extract_features_output.txt \\\n",
    "--find_target_selected_features extract_features_output.txt \\\n",
    "--find_target_fasta_file extract_seqs_output.fa \\\n",
    "--find_target_output_file targets.txt \\\n",
    "--find_target_iupac_mismatches 16:R \\\n",
    "--find_target_reference_fasta data/transcriptome_out.fa \\\n",
    "--find_target_max_errors 4 \\\n",
    "--find_target_Tm_min 58 \\\n",
    "--find_target_Tm_max 62 \\\n",
    "--find_target_lowest_percentile_Tm_score_cutoff 5 \\\n",
    "--find_target_min_dist_probes 8 \\\n",
    "--find_target_filter_ligation_junction \\\n",
    "--find_target_num_probes 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 DEBUG targets_df GC checked:\n",
      " Index(['Probe_id', 'Gene', 'Region', 'Sequence', 'GC', 'Coverage',\n",
      "       'Transcript_id'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing probes for specificity: 27408 alignments [00:32, 836.55 alignments/s]                \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m max_errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     12\u001b[0m check_specificity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m targets_df \u001b[38;5;241m=\u001b[39m \u001b[43mfind_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfasta_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfasta_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_fasta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreference_fasta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mplp_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mplp_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_coverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_coverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mgc_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_probes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_probes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miupac_mismatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miupac_mismatches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_specificity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_specificity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#python3 codes/find_target.py --selected_features extract_features_output.txt --fasta_file extract_seqs_output.fa --output_file targets.txt --iupac_mismatches \"5:R,10:G\" --reference_fasta data/transcriptome_out.fa --max_errors 4 --Tm_min 58 --Tm_max 62 --lowest_percentile_Tm_score_cutoff 5 --min_dist_probes 8 --filter_ligation_junction\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[237], line 946\u001b[0m, in \u001b[0;36mfind_targets\u001b[0;34m(selected_features, fasta_file, reference_fasta, plp_length, min_coverage, output_file, gc_min, gc_max, num_probes, iupac_mismatches, max_errors, check_specificity)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Check probe specificity against reference genome if requested\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_specificity:\n\u001b[0;32m--> 946\u001b[0m     specificity_results \u001b[38;5;241m=\u001b[39m \u001b[43mfind_probes_in_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_fasta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_specificity.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;66;03m#Debugging\u001b[39;00m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔹 DEBUG specificity checked:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecificity_results\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[237], line 803\u001b[0m, in \u001b[0;36mfind_probes_in_targets\u001b[0;34m(targets_df, reference_fasta, max_errors, output_file)\u001b[0m\n\u001b[1;32m    800\u001b[0m probe_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbe_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    801\u001b[0m probe_seq \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 803\u001b[0m adapter \u001b[38;5;241m=\u001b[39m \u001b[43mBackAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobe_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprobe_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m aligner \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39maligner\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Forward strand search\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/cutadapt/adapters.py:798\u001b[0m, in \u001b[0;36mBackAdapter.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_force_anywhere \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_anywhere\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/cutadapt/adapters.py:598\u001b[0m, in \u001b[0;36mSingleAdapter.__init__\u001b[0;34m(self, sequence, max_errors, min_overlap, read_wildcards, adapter_wildcards, name, indels)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindels: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m indels\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maligner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aligner()\n\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkmer_finder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kmer_finder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/cutadapt/adapters.py:810\u001b[0m, in \u001b[0;36mBackAdapter._kmer_finder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_kmer_finder\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_kmer_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mback_adapter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfront_adapter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_anywhere\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/cutadapt/adapters.py:633\u001b[0m, in \u001b[0;36mSingleAdapter._make_kmer_finder\u001b[0;34m(self, sequence, back_adapter, front_adapter, internal)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28mprint\u001b[39m(kmer_probability_analysis(positions_and_kmers))\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mKmerFinder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions_and_kmers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapter_wildcards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_wildcards\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Kmers too long.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MockKmerFinder()\n",
      "File \u001b[0;32msrc/cutadapt/_kmer_finder.pyx:120\u001b[0m, in \u001b[0;36mcutadapt._kmer_finder.KmerFinder.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/cutadapt/_match_tables.py:98\u001b[0m, in \u001b[0;36mmatches_lookup\u001b[0;34m(ref_wildcards, query_wildcards)\u001b[0m\n\u001b[1;32m     96\u001b[0m     query_table \u001b[38;5;241m=\u001b[39m _iupac_table()\n\u001b[1;32m     97\u001b[0m     comp_op \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mand_\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_matches_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomp_op\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/cutadapt/_match_tables.py:73\u001b[0m, in \u001b[0;36mall_matches_generator\u001b[0;34m(ref, query, comp_op)\u001b[0m\n\u001b[1;32m     71\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, query_char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(query):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m:  \u001b[38;5;66;03m# Only ASCII characters supported.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(comp_op(ref_char, query_char)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selected_features = '../extract_features_output.txt'\n",
    "fasta_file = '../extract_seqs_output.fa'\n",
    "reference_fasta = '../data/transcriptome_out.fa'\n",
    "plp_length = 30\n",
    "min_coverage = 1\n",
    "output_file = 'Candidate_probes'\n",
    "gc_min = 50\n",
    "gc_max = 65\n",
    "num_probes = 10\n",
    "iupac_mismatches = \"15:R,16:G\"\n",
    "max_errors = 4\n",
    "check_specificity = True\n",
    "\n",
    "targets_df = find_targets(selected_features = selected_features, fasta_file = fasta_file, reference_fasta = reference_fasta,\n",
    "                                 plp_length = plp_length, min_coverage = min_coverage, output_file=output_file, \n",
    "                                 gc_min=gc_min, gc_max=gc_max, num_probes=num_probes, iupac_mismatches=iupac_mismatches,\n",
    "                                 max_errors=max_errors, check_specificity=check_specificity)\n",
    "#python3 codes/find_target.py --selected_features extract_features_output.txt --fasta_file extract_seqs_output.fa --output_file targets.txt --iupac_mismatches \"5:R,10:G\" --reference_fasta data/transcriptome_out.fa --max_errors 4 --Tm_min 58 --Tm_max 62 --lowest_percentile_Tm_score_cutoff 5 --min_dist_probes 8 --filter_ligation_junction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = '../extract_features_output.txt'\n",
    "fasta_file = '../extract_seqs_output.fa'\n",
    "reference_fasta = '../data/transcriptome_out.fa'\n",
    "plp_length = 30\n",
    "min_coverage = 1\n",
    "output_file = 'Candidate_probes'\n",
    "gc_min = 50\n",
    "gc_max = 65\n",
    "num_probes = 10\n",
    "iupac_mismatches = \"15:R,16:G\"\n",
    "max_errors = 4\n",
    "check_specificity = True\n",
    "\n",
    "targets = []\n",
    "targets_df = []\n",
    "\n",
    "seq_dict = SeqIO.to_dict(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "selected_features = pd.read_csv(selected_features, sep='\\t')\n",
    "selected_features.index = selected_features['region']\n",
    "coverage_dict = dict(zip(selected_features['region'], selected_features['coverage']))\n",
    "\n",
    "if plp_length % 2 != 0:\n",
    "    raise InputValueError(\"The size of the probe should be an even number\", field=\"plp_length\", code=\"odd_value_for_plp_length_provided\")\n",
    "\n",
    "\n",
    "for keys in seq_dict:\n",
    "    gene, region = keys.split(\"|\")\n",
    "\n",
    "    chr_name, start_end = region.split(\":\")\n",
    "    initial_start = int(start_end.split(\"-\")[0])\n",
    "    seq = seq_dict[keys].seq\n",
    "    seq_len = len(seq) - (plp_length - 1)\n",
    "\n",
    "    coverage_value = coverage_dict.get(region, 0)\n",
    "    if coverage_value < min_coverage:\n",
    "        continue \n",
    "\n",
    "    for i in range(0, seq_len):\n",
    "        tmp_seq = seq[i:i + plp_length]\n",
    "        gc_content = (tmp_seq.count(\"G\") + tmp_seq.count(\"C\")) / len(tmp_seq) * 100\n",
    "\n",
    "        if not (gc_min <= gc_content <= gc_max):\n",
    "            continue\n",
    "        if not any(nucleotide * 3 in tmp_seq for nucleotide in \"ACGT\"):\n",
    "            continue\n",
    "\n",
    "        start = initial_start + i\n",
    "        end = start + plp_length - 1\n",
    "\n",
    "        targets.append({\n",
    "            \"Probe_id\": f\"{gene}|{chr_name}:{start}-{end}\",\n",
    "            \"Gene\": gene,\n",
    "            \"Region\": f\"{chr_name}:{start}-{end}\",\n",
    "            \"Sequence\": str(tmp_seq),\n",
    "            \"GC\": gc_content,\n",
    "            \"Coverage\": coverage_value,\n",
    "            \"Transcript_id\": selected_features.loc[region, 'transcript_id']\n",
    "        })\n",
    "\n",
    "targets_df = pd.DataFrame(targets)\n",
    "targets_df_cpy = targets_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running cutadapt on PLP_probe_design output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_targets(selected_features, fasta_file, reference_fasta, plp_length=30, min_coverage=1,\n",
    "                 output_file='Candidate_probes', gc_min=50, gc_max=65, num_probes=10,\n",
    "                 iupac_mismatches=None, max_errors=1, check_specificity=False, off_target_output=False):\n",
    "    \"\"\"\n",
    "    Extract target sequences based on defined probe criteria and optionally check specificity.\n",
    "\n",
    "    Args:\n",
    "        selected_features (str): Path to the selected features file (TSV format).\n",
    "        fasta_file (str): Path to the indexed FASTA file.\n",
    "        reference_fasta (str): Path to the reference genome FASTA.\n",
    "        output_file (str): Base path for the output files.\n",
    "        plp_length (int): Probe length (default: 30).\n",
    "        min_coverage (int): Minimum coverage of the region.\n",
    "        gc_min (int): Minimum GC content (default: 50).\n",
    "        gc_max (int): Maximum GC content (default: 65).\n",
    "        num_probes (int): Number of probes to select per gene (default: 10).\n",
    "        iupac_mismatches (str): IUPAC mismatch specification (e.g., \"15:R,16:G\").\n",
    "        max_errors (int): Maximum mismatches allowed during specificity checking.\n",
    "        check_specificity (bool): Whether to check probe specificity against reference.\n",
    "        off_target_output (bool): Whether to save off-target probe information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame with extracted (and filtered) probe sequences, off-target details or None)\n",
    "    \"\"\"\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    # Read the FASTA file and selected features\n",
    "    seq_dict = SeqIO.to_dict(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "    selected_features = pd.read_csv(selected_features, sep='\\t')\n",
    "    selected_features.index = selected_features['region']\n",
    "    coverage_dict = dict(zip(selected_features['region'], selected_features['coverage']))\n",
    "\n",
    "    if plp_length % 2 != 0:\n",
    "        raise InputValueError(\"The size of the probe should be an even number\", field=\"plp_length\", code=\"odd_value_for_plp_length_provided\")\n",
    "\n",
    "    for key in seq_dict:\n",
    "        gene, region = key.split(\"|\")\n",
    "        chr_name, start_end = region.split(\":\")\n",
    "        initial_start = int(start_end.split(\"-\")[0])\n",
    "        seq = seq_dict[key].seq\n",
    "        seq_len = len(seq) - (plp_length - 1)\n",
    "        coverage_value = coverage_dict.get(region, 0)\n",
    "        if coverage_value < min_coverage:\n",
    "            continue \n",
    "\n",
    "        for i in range(seq_len):\n",
    "            tmp_seq = seq[i:i + plp_length]\n",
    "            gc_content = (tmp_seq.count(\"G\") + tmp_seq.count(\"C\")) / len(tmp_seq) * 100\n",
    "\n",
    "            if not (gc_min <= gc_content <= gc_max):\n",
    "                continue\n",
    "            if not any(nucleotide * 3 in tmp_seq for nucleotide in \"ACGT\"):\n",
    "                continue\n",
    "\n",
    "            start = initial_start + i\n",
    "            end = start + plp_length - 1\n",
    "\n",
    "            targets.append({\n",
    "                \"Probe_id\": f\"{gene}|{chr_name}:{start}-{end}\",\n",
    "                \"Gene\": gene,\n",
    "                \"Region\": f\"{chr_name}:{start}-{end}\",\n",
    "                \"Sequence\": str(tmp_seq),\n",
    "                \"GC\": gc_content,\n",
    "                \"Coverage\": coverage_value,\n",
    "                \"Transcript_id\": selected_features.loc[region, 'transcript_id']\n",
    "            })\n",
    "\n",
    "    targets_df = pd.DataFrame(targets)\n",
    "\n",
    "    # Introduce IUPAC mismatches if specified and check ligation junctions. Or just check ligation junctions\n",
    "\n",
    "    targets_df = evaluate_ligation_junction(targets_df, iupac_mismatches=iupac_mismatches, plp_length=plp_length)\n",
    "\n",
    "    off_target_info = None  # default when specificity is not checked\n",
    "\n",
    "    # Check probe specificity against reference genome if requested\n",
    "    if check_specificity:\n",
    "        specificity_results = find_probes_in_targets(targets_df, reference_fasta, max_errors, \n",
    "                                                     output_file=f\"{output_file}_specificity.csv\")\n",
    "        off_target_file = f\"{output_file}_off_targets.csv\" if off_target_output else None\n",
    "        valid_targets_df, off_target_info = filter_probes_by_specificity(targets_df, specificity_results,\n",
    "                                                                        selected_features=selected_features,\n",
    "                                                                        off_target_output_file=off_target_file)\n",
    "        targets_df = valid_targets_df\n",
    "        print(f\"✅ Specificity results saved to {output_file}_specificity.csv\")\n",
    "\n",
    "    # Further filtering (e.g., select_top_probes) can be done here if desired\n",
    "\n",
    "    return targets_df, off_target_info\n",
    "\n",
    "def filter_probes_by_specificity(targets_df, specificity_results, selected_features, off_target_output_file=None):\n",
    "    \"\"\"\n",
    "    Filter candidate probes by verifying that all observed specificity results belong to the expected transcripts\n",
    "    for the gene (using the union of transcript IDs from selected_features).\n",
    "\n",
    "    Args:\n",
    "        targets_df (DataFrame): DataFrame with candidate probes. Expected to have:\n",
    "            - 'Probe_id' (format: \"Gene|chr:start-end\")\n",
    "            - 'Transcript_id' (semicolon-separated string)\n",
    "        specificity_results (DataFrame): DataFrame with specificity results. Expected to have:\n",
    "            - 'query_name' (probe id)\n",
    "            - 'target_name' (transcript id)\n",
    "        selected_features (DataFrame): DataFrame with selected features. Expected to have:\n",
    "            - 'gene_name'\n",
    "            - 'transcript_id' (semicolon-separated string)\n",
    "        off_target_output_file (str, optional): Path to save off-target probe information.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame of valid probes, list of off-target details)\n",
    "    \"\"\"\n",
    "    # Build gene-to-transcripts mapping from selected_features\n",
    "    gene_to_transcripts = {}\n",
    "    for _, row in selected_features.iterrows():\n",
    "        gene = row['gene_name']\n",
    "        transcripts = set(row['transcript_id'].split(';'))\n",
    "        if gene in gene_to_transcripts:\n",
    "            gene_to_transcripts[gene] = gene_to_transcripts[gene].union(transcripts)\n",
    "        else:\n",
    "            gene_to_transcripts[gene] = transcripts\n",
    "\n",
    "    valid_probes = []\n",
    "    off_target_details = []\n",
    "    off_target_df = None\n",
    "\n",
    "    for probe_id in targets_df['Probe_id'].unique():\n",
    "            # Assume probe_id format is \"Gene|chr:start-end\"\n",
    "            gene = probe_id.split('|')[0]\n",
    "            expected_transcripts = gene_to_transcripts.get(gene, set())\n",
    "            \n",
    "            # Get observed transcript IDs from specificity_results for this probe\n",
    "            observed_transcripts = set(specificity_results.loc[specificity_results['query_name'] == probe_id, 'target_name'].unique())\n",
    "            \n",
    "            # Check: if all observed transcripts are among expected transcripts for the gene, the probe is valid.\n",
    "            if observed_transcripts.issubset(expected_transcripts):\n",
    "                valid_probes.append(probe_id)\n",
    "            else:\n",
    "                print(\"matched: \", observed_transcripts & expected_transcripts)\n",
    "                print(probe_id, gene, expected_transcripts, observed_transcripts)\n",
    "                off_target_details.append({\n",
    "                    'Probe_id': probe_id,\n",
    "                    'Gene': gene,\n",
    "                    'Sequence': targets_df.loc[targets_df['Probe_id'] == probe_id, 'Sequence'].values[0],\n",
    "                    'Expected_transcripts': expected_transcripts,\n",
    "                    'Observed_transcripts': observed_transcripts,\n",
    "                    'Off_targets': observed_transcripts - expected_transcripts\n",
    "                })\n",
    "        \n",
    "    # Filter targets_df to only include valid probes\n",
    "    valid_targets_df = targets_df[targets_df['Probe_id'].isin(valid_probes)]\n",
    "\n",
    "    # Optionally, save off-target details to a CSV file\n",
    "    if off_target_output_file and off_target_details:\n",
    "        off_target_df = pd.DataFrame(off_target_details)\n",
    "        off_target_df.to_csv(off_target_output_file, index=False)\n",
    "        print(f\"Off-target probe details saved to {off_target_output_file}\")\n",
    "    else:\n",
    "        off_target_df = None\n",
    "\n",
    "    # Return the valid probes and the off-target details list (even if empty)\n",
    "    return valid_targets_df, off_target_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTF loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing GTF file....\n",
      "Loading GTF file....\n",
      "Processing genes: grik2\n"
     ]
    }
   ],
   "source": [
    "gtf_file = '../data/Mus_musculus.GRCm39.113.chr.gtf'\n",
    "genes_str = 'Grik2'\n",
    "identifier_type='gene_name'\n",
    "gtf_df, genes_of_interest = parse_gtf(gtf_file = gtf_file, genes_str = genes_str, identifier_type=identifier_type) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing probes for specificity: 11902 alignments [00:14, 807.45 alignments/s]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Specificity results saved to Candidate_probes_specificity.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Results saved to Candidate_probes_specificity.csv\n"
     ]
    }
   ],
   "source": [
    "selected_features = '../extracted_all_gene_features_output.txt'\n",
    "fasta_file = '../extract_seqs_output.fa'\n",
    "reference_fasta = '../data/transcriptome_out_tmp.fa'\n",
    "plp_length = 30\n",
    "min_coverage = 1\n",
    "output_file = 'Candidate_probes'\n",
    "gc_min = 50\n",
    "gc_max = 65\n",
    "num_probes = 10\n",
    "iupac_mismatches = None\n",
    "max_errors = 4\n",
    "check_specificity = True\n",
    "\n",
    "targets_df, off_targets_df = find_targets(selected_features = selected_features, fasta_file = fasta_file, reference_fasta = reference_fasta,\n",
    "                                 plp_length = plp_length, min_coverage = min_coverage, output_file=output_file, \n",
    "                                 gc_min=gc_min, gc_max=gc_max, num_probes=num_probes, iupac_mismatches=iupac_mismatches,\n",
    "                                 max_errors=max_errors, check_specificity=check_specificity, off_target_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_targets_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(48970929)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = targets_df['Region'].str.split(':').str[1].str.split('-', expand=True)\n",
    "tmp.columns = ['start', 'end']\n",
    "start = tmp['start'].astype(int)\n",
    "end = tmp['end'].astype(int)\n",
    "\n",
    "min_start = start.min()\n",
    "max_end = end.max()\n",
    "\n",
    "(min_start, max_end)\n",
    "gtf_df['start'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Probe_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m off_targets_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(off_targets_df)\n\u001b[0;32m----> 2\u001b[0m off_targets_df_counts \u001b[38;5;241m=\u001b[39m \u001b[43moff_targets_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProbe_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcounts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m targets_df_counts \u001b[38;5;241m=\u001b[39m targets_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProbe_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcounts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m      5\u001b[0m     targets_df_counts, off_targets_df_counts, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProbe_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m, suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_targets\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_off_targets\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/pandas/core/frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3.10.10/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Probe_id'"
     ]
    }
   ],
   "source": [
    "off_targets_df = pd.DataFrame(off_targets_df)\n",
    "off_targets_df_counts = off_targets_df.groupby('Probe_id').size().reset_index(name='counts')\n",
    "targets_df_counts = targets_df.groupby('Probe_id').size().reset_index(name='counts')\n",
    "merged = pd.merge(\n",
    "    targets_df_counts, off_targets_df_counts, on='Probe_id', how='outer', suffixes=('_targets', '_off_targets') \n",
    ")\n",
    "off_targets_df\n",
    "\n",
    "#off_targets_df[off_targets_df['Probe_id'] == \"Grik2|10:48977243-48977272\"][['Observed_transcripts']].values[0]\n",
    "#targets_df[targets_df['Probe_id'] == \"Grik2|10:49148932-49148961\"][['Transcript_id']].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ligation_junction(targets, iupac_mismatches=None, plp_length=30):\n",
    "    \"\"\"\n",
    "    Evaluates the ligation junction of a probe and introduces mismatches if needed.\n",
    "\n",
    "    Args:\n",
    "        targets (pd.DataFrame): DataFrame containing probe sequences in the 'Sequence' column.\n",
    "        iupac_mismatches (str or list of tuples): Mismatch instructions in the form \"5:R,10:G\" or [(5, 'R'), (10, 'G')].\n",
    "        plp_length (int): Length of the probe (default: 30).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with modified probes and ligation junction statuses.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame has a 'Ligation junction' column\n",
    "    if 'Ligation junction' not in targets.columns:\n",
    "        targets['Ligation junction'] = 'non-preferred'\n",
    "\n",
    "    # Determine junction position (using the original probe indexing)\n",
    "    junction_position = int((plp_length / 2) - 1)\n",
    "    new_rows = []\n",
    "\n",
    "    for idx in targets.index:\n",
    "        probe_seq = targets.loc[idx]['Sequence']\n",
    "        ligation_junction = probe_seq[junction_position] + probe_seq[junction_position + 2]\n",
    "        ligation_status = ligation_junctions_dict.get(ligation_junction, \"non-preferred\")\n",
    "        targets.loc[idx, 'Ligation junction'] = ligation_status\n",
    "\n",
    "        if iupac_mismatches is not None:\n",
    "            # If mismatches are provided as a string, parse them into a list of (pos, symbol) tuples.\n",
    "            if isinstance(iupac_mismatches, str):\n",
    "                iupac_mismatches = parse_iupac_mismatches(iupac_mismatches)\n",
    "                \n",
    "            # Limit to 2 mismatches\n",
    "            if len(iupac_mismatches) > 2:\n",
    "                raise InputValueError(\"The number of mismatches should be less than or equal to 2\",\n",
    "                                      field=\"iupac_mismatches\", code=\"mismatches_exceed_limit\")\n",
    "\n",
    "            # Convert user-provided (1-indexed) positions to 0-indexed for internal use,\n",
    "            # while preserving the original 1-indexed value for the probe ID suffix.\n",
    "            # Each tuple becomes (adjusted_pos, original_pos, iupac_symbol)\n",
    "            mismatches_converted = [(pos - 1, pos, symbol) for pos, symbol in iupac_mismatches]\n",
    "\n",
    "            # Try all combinations of 1 or 2 mismatches\n",
    "            for r in range(1, len(mismatches_converted) + 1):\n",
    "                for subset in itertools.combinations(range(len(mismatches_converted)), r):\n",
    "                    selected_mismatches = [mismatches_converted[i] for i in subset]\n",
    "                    # For each mismatch, retrieve the possible replacement bases from IUPAC_CODES.\n",
    "                    replacement_options = [IUPAC_CODES[symbol] for _, _, symbol in selected_mismatches]\n",
    "\n",
    "                    # Generate all possible replacement combinations.\n",
    "                    for replacement in itertools.product(*replacement_options):\n",
    "                        original_seq_list = list(probe_seq)\n",
    "                        modified_seq = original_seq_list.copy()\n",
    "                        new_id_suffix = []\n",
    "                        changes_made = False\n",
    "\n",
    "                        for (adj_pos, orig_pos, iupac_symbol), new_base in zip(selected_mismatches, replacement):\n",
    "                            original_base = original_seq_list[adj_pos]\n",
    "                            # Apply the replacement only if it results in an actual change.\n",
    "                            if original_base != new_base:\n",
    "                                modified_seq[adj_pos] = new_base\n",
    "                                new_id_suffix.append(f\"{orig_pos}_{original_base}_{new_base}\")\n",
    "                                changes_made = True\n",
    "\n",
    "                        # Only add a new probe row if at least one change occurred.\n",
    "                        if changes_made:\n",
    "                            new_probe_seq = \"\".join(modified_seq)\n",
    "                            new_probe_id = f\"{targets.loc[idx, 'Probe_id']}|{'_'.join(new_id_suffix)}\"\n",
    "                            \n",
    "                            # Re-evaluate the ligation junction with the new sequence.\n",
    "                            new_ligation_junction = new_probe_seq[junction_position] + new_probe_seq[junction_position + 2]\n",
    "                            new_ligation_status = ligation_junctions_dict.get(new_ligation_junction, \"non-preferred\")\n",
    "\n",
    "                            new_row = targets.loc[idx].copy()\n",
    "                            new_row['Sequence'] = new_probe_seq\n",
    "                            new_row['Ligation junction'] = new_ligation_status\n",
    "                            new_row['Probe_id'] = new_probe_id\n",
    "\n",
    "                            new_rows.append((new_probe_id, new_row))\n",
    "\n",
    "    # Append the new rows to the DataFrame, if any.\n",
    "    if new_rows:\n",
    "        new_rows_df = pd.DataFrame([row[1] for row in new_rows], index=[row[0] for row in new_rows])\n",
    "        targets = pd.concat([targets, new_rows_df])\n",
    "\n",
    "    return targets\n",
    "\n",
    "def select_top_probes(df, num_probes):\n",
    "    \"\"\"\n",
    "    Select the top probes based on coverage and ligation junction preferences.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame with probe sequences.\n",
    "        num_probes (int): Number of probes to select.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with the top probes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the dataframe: Highest coverage first, then ligation junction order\n",
    "    df_sorted = df.sort_values(by=[\"Coverage\", \"Ligation junction\"], \n",
    "                               ascending=[False, True], \n",
    "                               key=lambda col: col.map({'preferred': 0, 'neutral': 1, 'non-preferred': 2}).fillna(3))\n",
    "    # Remove probes non-preferred ligation junctions\n",
    "    df_sorted = df_sorted[df_sorted['Ligation junction'] != 'non-preferred']\n",
    "    \n",
    "    # Select top `num_probes`\n",
    "    final_df = df_sorted.groupby(\"Gene\").head(num_probes)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def create_fasta(targets_df, output_file):\n",
    "    \"\"\"\n",
    "    Create a FASTA file from the DataFrame with probe sequences.\n",
    "\n",
    "    Args:\n",
    "        targets_df (DataFrame): DataFrame with probe sequences.\n",
    "        output_file (str): Path to the output file.\n",
    "    \"\"\"\n",
    "    output_file = output_file + \".fa\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _,row in targets_df.iterrows():\n",
    "            f.write(f\">{row['Probe_id']}\\n{row['Sequence']}\\n\")\n",
    "\n",
    "## Ligation junction dictionary\n",
    "ligation_junctions_dict = {'TA': 'preferred',\n",
    "                        'TA': 'preferred',\n",
    "                        'GA': 'preferred',\n",
    "                        'AG': 'preferred',\n",
    "                        'TT': 'neutral',\n",
    "                        'CT': 'neutral',\n",
    "                        'CA': 'neutral',\n",
    "                        'TC': 'neutral',\n",
    "                        'AC': 'neutral',\n",
    "                        'CC': 'neutral',\n",
    "                        'TG': 'neutral',\n",
    "                        'AA': 'neutral', \n",
    "                        'CG': 'non-preferred', \n",
    "                        'GT': 'non-preferred',\n",
    "                        'GG': 'non-preferred',\n",
    "                        'GC': 'non-preferred'}\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "## Evaluate IUPAC mismatches format:\n",
    "def parse_iupac_mismatches(mismatch_str):\n",
    "    \"\"\"\n",
    "    Parses a string of mismatches formatted as \"pos:base,pos:base\" into a list of tuples.\n",
    "    \n",
    "    Args:\n",
    "        mismatch_str (str): Mismatch input string (e.g., \"5:R,10:G\")\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of (position, base) tuples, e.g., [(5, 'R'), (10, 'G')].\n",
    "    \"\"\"\n",
    "    mismatches = []\n",
    "    try:\n",
    "        for pair in mismatch_str.split(\",\"):\n",
    "            pos, base = pair.split(\":\")\n",
    "            pos = int(pos.strip())  # Convert position to integer\n",
    "            base = base.strip().upper()  # Ensure base is uppercase\n",
    "            mismatches.append((pos, base))\n",
    "\n",
    "    except (ValueError, IndexError):\n",
    "        raise InputValueError(\"Invalid format for --iupac_mismatches. Use 'pos:base,pos:base', e.g., '5:R,10:G'.\", \n",
    "                              field=\"iupac_mismatches\", code=\"invalid_mismatch_format\")\n",
    "    \n",
    "    return mismatches\n",
    "\n",
    "def find_targets(selected_features, fasta_file, reference_fasta, plp_length=30, min_coverage=1,\n",
    "                 output_file='Candidate_probes', gc_min=50, gc_max=65, num_probes=10,\n",
    "                 iupac_mismatches=None, max_errors=1, check_specificity=False, off_target_output=False):\n",
    "    \"\"\"\n",
    "    Extract target sequences based on defined probe criteria and optionally check specificity.\n",
    "\n",
    "    Args:\n",
    "        selected_features (str): Path to the selected features file (TSV format).\n",
    "        fasta_file (str): Path to the indexed FASTA file.\n",
    "        reference_fasta (str): Path to the reference genome FASTA.\n",
    "        output_file (str): Base path for the output files.\n",
    "        plp_length (int): Probe length (default: 30).\n",
    "        min_coverage (int): Minimum coverage of the region.\n",
    "        gc_min (int): Minimum GC content (default: 50).\n",
    "        gc_max (int): Maximum GC content (default: 65).\n",
    "        num_probes (int): Number of probes to select per gene (default: 10).\n",
    "        iupac_mismatches (str): IUPAC mismatch specification (e.g., \"15:R,16:G\").\n",
    "        max_errors (int): Maximum mismatches allowed during specificity checking.\n",
    "        check_specificity (bool): Whether to check probe specificity against reference.\n",
    "        off_target_output (bool): Whether to save off-target probe information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame with extracted (and filtered) probe sequences, off-target details or None)\n",
    "    \"\"\"\n",
    "\n",
    "    targets = []\n",
    "\n",
    "    # Read the FASTA file and selected features\n",
    "    seq_dict = SeqIO.to_dict(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "    selected_features = pd.read_csv(selected_features, sep='\\t')\n",
    "    selected_features.index = selected_features['region']\n",
    "    coverage_dict = dict(zip(selected_features['region'], selected_features['coverage']))\n",
    "\n",
    "    if plp_length % 2 != 0:\n",
    "        raise InputValueError(\"The size of the probe should be an even number\", field=\"plp_length\", code=\"odd_value_for_plp_length_provided\")\n",
    "\n",
    "    for key in seq_dict:\n",
    "        gene, region = key.split(\"|\")\n",
    "        chr_name, start_end = region.split(\":\")\n",
    "        initial_start = int(start_end.split(\"-\")[0])\n",
    "        seq = seq_dict[key].seq\n",
    "\n",
    "        if re.search(r'(A{3,}|C{3,}|G{3,}|T{3,})', str(seq)):\n",
    "            continue  # Skip this entire sequence\n",
    "\n",
    "        seq_len = len(seq) - (plp_length - 1)\n",
    "        coverage_value = coverage_dict.get(region, 0)\n",
    "        if coverage_value < min_coverage:\n",
    "            continue \n",
    "\n",
    "        for i in range(seq_len):\n",
    "            tmp_seq = seq[i:i + plp_length]\n",
    "            gc_content = (tmp_seq.count(\"G\") + tmp_seq.count(\"C\")) / len(tmp_seq) * 100\n",
    "\n",
    "            if not (gc_min <= gc_content <= gc_max):\n",
    "                continue\n",
    "\n",
    "            start = initial_start + i\n",
    "            end = start + plp_length - 1\n",
    "\n",
    "            targets.append({\n",
    "                \"Probe_id\": f\"{gene}|{chr_name}:{start}-{end}\",\n",
    "                \"Gene\": gene,\n",
    "                \"Region\": f\"{chr_name}:{start}-{end}\",\n",
    "                \"Sequence\": str(tmp_seq),\n",
    "                \"GC\": gc_content,\n",
    "                \"Coverage\": coverage_value,\n",
    "                \"Transcript_id\": selected_features.loc[region, 'transcript_id']\n",
    "            })\n",
    "\n",
    "    targets_df = pd.DataFrame(targets)\n",
    "\n",
    "    # Introduce IUPAC mismatches if specified and check ligation junctions. Or just check ligation junctions\n",
    "\n",
    "    targets_df = evaluate_ligation_junction(targets_df, iupac_mismatches=iupac_mismatches, plp_length=plp_length)\n",
    "\n",
    "    off_target_info = None  # default when specificity is not checked\n",
    "\n",
    "    # Check probe specificity against reference genome if requested\n",
    "    if check_specificity:\n",
    "        specificity_results = find_probes_in_targets(targets_df, reference_fasta, max_errors, \n",
    "                                                     output_file=f\"{output_file}_specificity.csv\")\n",
    "        off_target_file = f\"{output_file}_off_targets.csv\" if off_target_output else None\n",
    "        valid_targets_df, off_target_info = filter_probes_by_specificity(targets_df, specificity_results,\n",
    "                                                                        selected_features=selected_features,\n",
    "                                                                        off_target_output_file=off_target_file)\n",
    "        targets_df = valid_targets_df\n",
    "        print(f\"✅ Specificity results saved to {output_file}_specificity.csv\")\n",
    "\n",
    "    # Further filtering (e.g., select_top_probes) can be done here if desired\n",
    "\n",
    "    return targets_df, off_target_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq AGGTCCTTCTGTAGCGCCATGGTGGAAGAACTGAGAATGTCTCTGAAGTGCCAGCGTCGGCTCAAACATAAGCCACAGGCCCCAGTTATTGTGAAAACAGAAGAAGTTATCAACATGCACACATTTAACGACAGAAGGTTGCCAGGTAAAGAAACCATGGCATGAAGCTGGGAGGCCAATCACCCAAGCACAAACTGTCGTCTTTTTTTTTTTTTTCAAACAATTTAGCGAGAATGTTTCCTGTGGAAATATGCAACCTGTGCAAAATAAAATGAGTTACCTCATGCCGCTGTGTCTATGAACTAGAGACTCTTGTGATCTAAGCAGTTTCAGTGATCAGACTTGATTTACAAGCACCATGGATCGACAAAGTTACACGGGGTTACACTGTTTATCATGGGTTCCTCCCTTCCTTTGAGTGAATGTTACATGAAAATGTTGTGGCTGGTTTCAAATGCAGTCCAGAGAGAAACTGCTGGTTCCTTCTGAAGCTCAACTGTTGTCAGGAGATGGAATGTTGGGGCCCAAAAGGATAACCAATAAAAATGCCATAATTTATAAAAGCAAAACAAAAAGCGTGTGAAATCTGCAAAAATTGTAGTGTCACAAGAAACAGTATAGTCCCATGGTCACCAACAAAATGAGGTGATAATGTTACTAGCCCCCAATACTCAGTAAAATCATCATCTGAATAGATAATGTGTTCATAGAATGTGGAAAAAATGTAATGCAAAACATATCAGTATTCAATCAAAGTGGAACAGAAAGCAGACCACCATCAGTTATTTTCCTTTCTCAATAGTCTGTGTCATGGATTGTGATATAGATGGCAATTATCTATCTAATTGTTTTCTTAAAATACCCATGGCAAATATTTTAAAATGCAACTTGCTCCCAGGAACCCCTACCCTAACCTACACTAGAAATAAAAAAGCCACCACTGGTATAAAGATTCTGATGTAAAAGATATGTTTTTCAATCCTTGTCATGAATTGTAAAACAGGGCTCAGTATTACTGGTTATATGGAAGACTGAAGCTTTCACTCTGACATTCTGATATGTCAGCTGAAACTCTCCTTCCTCCTGGAAAGGACCTTGATGGAGCCTGGGCAGATTCCATTGATAAGACTGGGGACTTGTCACCTATACAGAACTACGTGACAGAACTTTGAGGTGGACTGCATTTAACAATAGTCACAATGTTAAAAGAACAAAATTCTTGAGCAGTTTTTTTTTTCTGTTTTGTTTTAAAAAATGTTCAGGTTTATTTGTGGAAATGCAAGATTTCTATGAAAATAGTTTTTGTATGGAAATTTTTGTAATACTTTTTATCAACAAAATAAGAACATGTGTTCCTGTCAGGGGTGTGATGTCAAGCATGAACGGTAGTGCGTGTGCACCACCAACGTTTGGTGAAAACTATTTTTATCAAGAAAAAGGAATCATAGAAGAGAAATATTTTCAAGTTAGATACTATAAAAGCTAGGTGCACTACCACCACGGCTTGTCACGCCACACCCCTGAGTCCCACAAGGCAGATAACATATTGTAATGAACAGTTGTGTGTAAAATGATAAAAGACACAGACCTCTTGACAACATTGTGAAAACAGTTGAGTGCACACAGTTTGCTGTTTGAATCCAATGCACAAAAATTTTACAAAACTCCATTAAAATTATGTCCATTTTACTTTCAGCTTTGGCTTTGATTTTTCTCTTGCATGTGTAAATGAATGTAACATGGTGGTTTTGTATAGAAAATATACATCAAGGGGTCTTAGGATCTCAAAGTTAGAATCTTCCCAACTTACAGCAAAAAGGAAAAGGCCATCCTGGAGGTGCTCCTCTCTTTCTCTCTCCCTTCCTCTGTCTCTCTCTTTGACTCTGTCTCTTTGTGTCTCAGTCTTTCTCTATATCAGTTTTTCTCTGTCCCTCTCTACCTCTGGCTCTATCACTCTCTGTCTCATTTACACACATACACACACACACACACACACACACACACACACGAATAAAGACATATACATTGGTTTTAGAATTAGGGTAGCTGGAATAAAAAGAATATGATTGTAGAGATGGCAACCTTTATCTTATCTCATTTGTAGCTGGAAATTGACTAAGTTCACTGTGCTGCATTATGTTGTGGAATGGTAATTATCTACTTTGGTTCAACTCATATCCAATTTCAGAATTTTCTGTGCATTGATACTTCAATAATCATCAGCAGAGGAACAAAAAGGGAAAAGTTTAGAATTAATAATTAATTTTAGATCCTAACATATTAATAGAAACAAACTATAACAGTTTTACGTTTTGAAAATCAAATCTGTAAGATTCAACTTATTTTCCTGATTAATTAATTAATTAATTCTAAGTGTGCAATTATAATTGGAATCTGACAAAAAAAAAACCCCACTGGAAAAGTTTCCATAATGTATTTCTTAAAATAGTAAAAATTGCATAATCAAATTATCTCAAATTAATTAAGATTTATATATGTGAGCACTTTAAATATTTTATGCTATGATTATTATCAGATTTCAATGATTATTTTGTTCAAGCTACAAATGTAGCTATACAAATCACTGCTAAAGTAGCAGTACTGGTGTATTAGTGCCAACCAAGTTTAAATAAGGAAAATAATATAAGTATTCAGATTATTAAGATGCTGTTTTTAACAAACAAATTTTAAATTTTATGAAAATATGAATTTGTAAAGGAAACAAACTTCATTATTAATATTATGGGGAAATTCTGTGAATATATACAATCTGACACATGTAGAATTTGCACATTCGATGAGAAACTGTGTCAAAAATGATCAATTGAAGCAACTCATTTAATAAAAAAGAACTCTCACTAAGCAATGCTTTAATATGTTTTAAAGATATAATTTTAAACACTTGGAAATCTTATATATGTGAGTATAAAAACACATTAAATATTCATATCTTAATTTACTTAAAAGAGTTTTAACTTCAATTTATTACTGAATTTAATAATCATAAACATTGGGTATAAAATTACAACTTTATTGTTAAGCAGCCAGAAGAGAATGAATTCTGGTAAAATTTGATAATGATTTTATTGAAAAATTGAATTAAGTTCTTAAATATGTAACAACCTTACTAGACATTAACTTTATAAATGATATTAAATGTTTATAATATATTCTTTTACATTCTAATTCTAATTAATACTTCATACATGTATAAGTACTTAAATTTTCCAAAACACCTGGTCATAATATATATATTAATTTTAATTATAATAAAAATGCTAATTACTTTCATACATAATATCAAACAATGAAAAAACTCTAGGTTGGAAGCATGTAAGAGTTCTCAGCATTTTCTAGAGGAAAAATCAAAAGCAAAGAGAAGCTAATACCTGTTCTAGGCAACATCAGAACCTATGAATTGAGAGGATGAATGGAGGCTCATGGACTTCAGCTATAGAAACCAACAAGGATGGAGACACCAGAGGTTTTATACCCACATTTACTACCACTAAATAATCAGTATCTCCCTGGGAACACCAAAGAAACACACATAATTACCACTGAACAACCCTAACTCTCATGGGAACACAAGAGAAATATAGTCTAGTTTATAAATGTGTAAAATGAAGAATAGTTTATTGATGGGAATATTTCAGATATTTTGTTGCTCACTTATATCTACATATAAGTGCATATGTAATAAAAATTCATATCCCATGGCACCATTCTTGGGTGGTATAGATATGAAACATTGCTGGTTTCAAAATGTTTAAGGATTGTGTCAACTTTTGATGTCTGTTTACTTTGAAATATATTGTAACTGCAGAAAAGTGCATATACAAGGCATATATACACAGAATTGCTGCTAGTTGTTATAGTTTTAAACACAGTTATTGTGCCATTTAACCCAGGCTCAATGAAAGTTCTCTGACTTGCTGATAACATTTTCAGGGAAGAAATTACAGTGATCTTTGATCATGAAATGTATAATTAACTCATCTGTGTTTGACTATGCCAATAGAGTGTGTACAGCATTGTAGAGAAACAGTCTTGGGATTTTGTGGCAGCTTTTAAAGGTAGAATCATAGCAATGAAAACTTTGGTTATGCCTGTCCTACCTAAATGTATACAAAATTTGAACCATTTTCCTCCATTGTGAGCTGATACGTGACCAGATCTGCAGACATGTAAAATTGTGAAGTTATTCCCTTGTCTTTAGGTATTTGCCTCAATGCAAATTCAAATGTAATAATTTTTTTTATTTCTCTTAAAATATGTCAGTTAATTATTTGATTTTTTCTCAATTTTGGTGGTTTTATAGAAATGCTGAATAGGCTCAATGGGTTTTAGATGTTTTATGTCTATTATTCTACTCTAGTCTATAATTATCTGGATATTCATTGACTCCTAGCTCAAAATTAGTTCCTTGGAAAATTTGAAGACTTTCCACTATTTCCTCTTCCCCCTCATCTATTTCCCATTTTCTCCCCCATTTCCATCTCTTTCCCCCATTCCTCATCCTTTATATATATATATATATATATATATACATATATATATATATATATGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTACTTGCTTTTTTATTATCTTGTCTAATGTTGAGATTGCTGTGGAAAACCTAAAATCATGATCAGATGGTCACAGTTCTTTGATCATCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCTTCCTCTTCCTCCTCCTCCTCCTCTTCCTCCTCCTCTTCCTCTTCGTCGTCTTCTCCTTCTCCTCCTTCTTTTCCTTCTCTTCCTCCACCTCTTCCTTTCTTCTTCTTTTCCCTCTTCTTCCTCCTTCTCTTGTGCCTCCTCTTTTTGTTTTTCTTACTTCTCTCTTTGTCTCTATCTCTATGTTTCTGTTTCTTTGTCTCTGTCTCTCTCTCTGTGTCTCTGTCTGTCAGTCTCTGTCGGCCTGTCTGTCTTTCTCTCTGTCTCAGATGTAGATTGATCTGTATCAATTGTACTAACCCTTACTAGACTCCTTAATAAAGCTACTATTTTTCAATTTTAAGAAAAGACCTTGAAATAATACTTCATTGTCTTCCTTTCTTCATATTTCCTTTGTTCTCTCTAAATCTGTTCTTATGACTTGTAATTACTTAACTTTGAATCTTCAGTCTATTTACTTTTCTATTTCTCTATATTGTTATTTTAAGTCTATAATCTAAAATACTTTTTTAGTTTCTCCTATCTTTGACATTGTTTTAATTTCTGTAAGTTACTGTTTAATTCCTATGACATCTTCACTTATATCATAGTAGTTGATCATAATTGTTCTTTATTCCCATCATTAGGAAAAGACATTGTATATTCCTGTTCTTTTCTATTTTTTTCACCTGAGTTGAAGGTATTATTGAGATAACAGTTGAGTCATTTACCCTCCAGTTACTCCTCCCAGTTCTTTTCACTTTCTCTCCCACATGTATCTACTTCCTGTTTGTCTCTCATTCAAAAAGGAAAGGCCTCTAAGAGATAACAGCAAAATTTAACAACATAAAATATAATAACATAAAACACACAAGCAAAAACTGTCACATCAAAGTTGAACAAGATAAACCAACAAAACAAAAAGAGCCTCAAGAAGGCAGAAGAATCAGAGAGCCTCTTGTTCACTTACCTCAGTTGAATTAGAATTGGTTCTTATCCTGTGGGTAGTGACCCATTTAGAGGTAGAATGACCCTTTCTTAAAGTTTTCCTAAGACAATCAGACAATATAGATGTTTACATTATGATTTATAAATAGCAAAATTATAGTTATGAAATAGAGGCAAAAATAATTTTATGGTTTTGGGTCATTACAACATTAAGAACCATATTTGAGGGTTCTAGCCTTAGGAAGGTTGAAAACCACTGAATTATAAAGGGCCTTGTCTCCTTGTGTCCTCTACCCTTTTGAATCTTTCACATTTCTCACCTCATCTTCTGTGGGTGGGATTTAGTGGAGACATCCCACTGAAAGCTGTGTTCCAAATGCATCTGTTTGTTAAATGTCATCCGTCAACTCTTACTGTTCTCTTTATGATGATGTAAAGTAGAATAAACATCACTGGCAAATTTTTGTTTATGTTAAGTTGTGACAGAATGTTCATCTAACTTGAGTAATCCTGTAGGTGAATATATGTGGACTACTTGATGAATAATCTGTCCAAGTACCTCCGTTTGGTTTTTACTTCTTGGGTATATTAGATTTGCAGAGAGGTGTCTTTTAACTTCATTCCTAATGCCTTGAAATCCTGATCACTATATTTCAGCCACCAGAGTCAAGAACTTATCACTGCAAATTCTGCCCTGTGTGTGAGTGAATAAAGCAATTTTTGTCCCATTTATTCTGAAAA\n",
      "seq TTTTTTTCCATGAAAACAAGCTACTTAGCCTATTCTTTTAATATAAAATTAATGTTTCCACTTTGTTATAAAACTTATATGATCAAATAGGCTGAGGATAAGATGGCATGCCATGTTAAACATTAGTTGATATTTAGGAACTCTGTATTAAATAAAGCCATCTTTCAAACAATTGAAACCATTCACACTTTAAACCATTTTTAAATGTCAATGGGATCAACTTCTAACCTTGTAATAATAACTGTATTTTATTAATTGATTCTTCTATTAAATGTTTCAACAGAAAGTCAAGACTATCTTACCTCAAGACAATGTATTCCTTCCCCTTTTGGAATCAGATTTTTCCATTTCTACCGTGTTGTCATCATCACCATCTTCATCATCATTATCATCATGTTGTTAATAAATACATCTAATAGAAG\n",
      "seq GAATCTTCTATTTGGTTAGTGCCACCATACCATCCAGACACTGTTTAGTAAACTTTTGAAACTTTCTAAAAGAGGTTTTTAATGATG\n",
      "seq GTTCTCCATATCGAGACAAAATCACCATAGCCATTCTTCAGCTGCAGGAGGAAGGCAAGCTGCACATGATGAAGGAGAAGTGGTGGCGAGGCAATGGCTGCCCAGAGGAGGAGAGCAAAGAGGCCAGTGCTCTAGGGGTGCAGAATATTGGTGGTATCTTCATTGTCCTGGCAGCCGGCTTGGTGCTCTCAGTTTTTGTGGCAGTGGGAGAGTTTTTATACAAATCCAAAAAAAACGCTCAATTGGAAAAG\n",
      "seq AAATCAAAAATCTCAACGTATGATAAAATGTGGGCATTTATGAGCAGCAGGAGACAGTCTGTGCTTGTCAAAAGCAATGAGGAAGGGATTCAACGTGTCCTCACCTCCGATTATGCTTTCTTAATGGAGTCAACGACCATCGAGTTTGTTACCCAGCGGAACTGTAACCTCACGCAGATTGGTGGCCTTATAGACTCCAAAGGCTATGGTGTTGGCACTCCCATGG\n",
      "seq TTCTGCTCAGCAAATGGAACTACCAGAAGCTCACAGAGACAACCTGCAGAACAGTGGCAAAACTTTCCCAGGTGTATTTCAGACAAGGGGCTAAAGTCAAGAATTTACGAAGAACAGAAATGAAATTCCACAGGGGAGAAAAAGGAAAAAGGGCAGTAAACTCCTACCACACAGATTATTGTAATACTGTTTACTGATGCTCTTTTACTATAGCAAGGAAGTAGAACCAACCTAGTCCTTCAATGAATTGATAAATAATGAAA\n",
      "seq TTTTGGCATCCCTTCCTAAATAAAACTGATGAAGAGTCAATAGTTGCATTTTGCCTTTCCTCCATTTGGGGTCTAAGGTAAGGGCTTCTTTCCTCATTGGGCTGACAACATTGGAGTTCTCTCAGGTCTGAAGGGATACACTTTGTAAAAAATAGAGTGGAGTAGTGTAGTCTACTCAGGTAATCTGCTGTTAGCCTCAGGAGTCTGCAGTAACTAATCTTCACAATCAAGGTTAGACCATAATAGCAATGTTCACTTTTAAATTTAGCTACGAACTTGAGCAGTAAAATATTCTTTTTTGGAATTTGTATTTCCTAAAATTTTTGGAAATCTTTGAAAGTTGGTCCCCATTGGCAAGGTGAACTTGATTATTAGTGATAGCAATTATATGTCTATCTGTACCATCTATAAAGCTAAGTGATTAAAGCAATGGTAAATTCAGATATACATTTGCTTGGCCATGTCCTCACTTCACGGCTGGTGGATACATGAAAGAGGAACCCATCCTATCTGGCTGATGCCCCGATCCCCAGGAATTAGTGGGATATAGTGCTGGACATGACGTTGCCTTGAGTACATGACAGTCAGCCCCTAACCAGGCTCTGATTGACAAGCCTAGAAGGTGTCTGCTGAATTTCAGTCTGTGGGTAAGATACAAACTTGTGGTAATTGGGACAGCCTTGTCAAATGTCTCCATCCACTATGTTTTTATTTCCTAGTCTCTTTGACTTTGTTGGTGTTTTATTTTTAAATAAACAAACAGTGAAGTATGCAGCCAAAGCTGCTGATCGTCTTGGGTGTGTTAGCGTTTCAGTATCTGAGTGTGTATCTAGCAGGGACTAGGAGCAACTACATCAGCAAGGTACCTTTCTGTTTTGTTTTGTTTTGTTTTGCTTTGCTTTGCTTTGCCCTTTTTTTCCTTTACAGAGTGGTAGACAAGAGTCTGGAGCTGGAGTTGGCTCTCATGCCCAAGTCTGTCTGCTTTTAAGATTTTTGTATCTTTGAAATTGCTAGGGTAAGATGAACTGAAGTGTAAGATGGGCCATTTTATATAAGACAGTTATAATAGCACATGTTTTCAATGTTAGCATGAAATATTTGTTTTTCTTTTCATCACTTCTAAAAAGGCATTCTCTCACTGCACAGAATTACCATTAAAGATTAGACTGTAAAACACATCTGCAAAATGAGTCCATCTTTCCTTGGTTACATTTTACTCAAGCCCTATAAAATATTAACCACTTCATAATCATGCTTCTCTGATTATAGATTCAAAATGATTAAAAGTGGATGGCACTTTATTTAATATATATTATATAATATATATATGAATATACTATATACTTTAAACAAGAATACCATATTCTGATATAAAGTTACAAATAATTAAACTTTAAATAAGAGAAAAAAATTCACATTAAAAAATTTTATACACTAAATAACCTAAAATTAATAATAATAGAAACTAAAGAATATTTCTCAGATACCATTCTCCTCTTAATATTATGGGCAGGCAACTCCAGCCATCTAACTGTGATGTATTTATGTAGTACATGTATAAAGATAAATTTGTGTCCATTTACTCACTTAATTGCCTTCATGGAGGCTAACCTTCAGAATGAGAAATATTTACTTAAACACAAGTAAATAATATGTTACTTTATTTTAAGATAAGCACAGGACTAATACATTAGACTTTGTAAAGTCTGTGATGAAATGAAATATTTTATTACCATACAATATTGTTATTTTAAATGAAGTATTGCTTCTTTGCTATTTACCTGAAATCAAAGGTAAATAATGAAGCAGACATATTTGAGTTCTTTAAAAAAGACCTTTTGTAAACTTATTAAAAAAAAAACTGCACTGAGTCTACATTGAATAATTACTTCTGAAAACCACTTAAATTATCAAATAGTAACATATGCATGTGGTTTATAAACTAAAGAGGTTTTGTGCTCTGATATTCTAATGTTAAGTTTCTAAGACTTTTTGCACATTGATATTTTAATTGTCACATGGCACCCTCTCAGGTAAGGCTAATCCCGTGTCATAAGTGGCATAGCAGTATAACTGGCCTCTGCTCTTCCGAGCAATGTTGCTTGTTTGCTGAAGCAGTGTGTCAATCTTTGACTTAAATTCTATTAGACAGTGATCCTGTTCTTCCACTATGACAAAGCATGTCTGACTACTGATCAAACTGTCTGTATTCGTTTCACCTTTCCCCCACTCTCTGTTAGGTTCTGAGCTCATGCCCAAAGCACTCTCCACCAGGATAGTGGGAGGCATTTGGTGGTTTTTCACACTTATCATCATTTCTTCGTATACCGCTAACCTAGCCGCCTTTCTGACCGTGGAACGCATGGAGTCGCCTATTGACTCTGCTGACGATTTAGCTAAGCAAACCAAGATAGAATATGGAGCAGTAGAGGACGGCGCAACCATGACGTTTTTCAAG\n",
      "seq GTTTAGTCCCTATGAGTGGTATAATCCACACCCTTGCAACCCTGACTCAGACGTGGTGGAAAACAATTTTACCTTGCTAAATAGTTTCTGGTTTGGAGTTGGAGCTCTCATGCAGCAAG\n",
      "seq AAAGCTGACCTTGCAGTTGCTCCACTGGCTATTACCTATGTTCGTGAGAAGGTCATCGACTTTTCAAAGCCGTTTATGACTCTTGGAATAAGTATTTTGTACCGCAAGCCCAATGGTACAAACCCAGGCGTCTTCTCCTTCCTGAATCCTCTCTCCCCTGATATCTGGATGTATATTCTGCTGGCTTACTTGGGTGTCAGTTGTGTGCTCTTTGTCATAGCCAG\n",
      "seq GAAGAACCATATGTCCTGTTTAAGAAGTCTGACAAACCTCTCTATGGGAATGATCGATTTGAAGGCTACTGTATTGATCTTCTACGAGAGTTATCTACAATCCTTGGCTTTACATATGAAATTAGGCTTGTGGAGGATGGGAAATATGGAGCCCAGGATGATGTGAATGGACAATGGAATGGAATGGTTCGTGAGCTAATTGATCAT\n",
      "seq ATTGGGACTTGGGATCCATCCAGTGGCCTGAATATGACAGAAAGTCAGAAAGGGAAGCCAGCAAATATTACAGATTCATTGTCTAATCGTTCTTTGATTGTTACCACCATTTTG\n",
      "seq GCTCATTGGGAAGGTCTTACAGGCAGAATTACATTTAACAAAACCAATGGATTGCGAACAGATTTTGATTTGGATGTGATCAGTCTCAAGGAAGAAGGTCTGGAGAAG\n",
      "seq ACTGATGCTGCTCTGATGTATGATGCAGTGCACGTTGTGTCTGTAGCTGTCCAACAGTTTCCCCAGATGACAGTCAGCTCCTTGCAATGCAATCGACACAAACCCTGGCGCTTTGGGACTCGCTTCATGAGCCTAATTAAAGAG\n",
      "seq GAGGCTTCCAACGGTAGGACTGGGACATCAAACCAACCACAATGACTGGTTCAACTTGAGGCCCACGTCATGAGAGGAAGCCCAGG\n",
      "seq GACAGATTGAAACAGCCAAATAATTGTTTTCCAAAAGAACCTGAAGAGACCATCTCTAATAGATAGACATGGCTTCCAGTGGCGGAATGGGGCCACCCACCCATCAAAAATGTTTGACCCAGAATTGTTCCTGTCTAAAGGAAATGCAGGGACAAAATTGGAACGAAGACTGAAGGAACAGCCATTCAGAGACTGCTCAAACCGGGGATACATCACATGCATAGTCACCAGTCTCCAACACTGCTGCTGATACCATGTTGTGCTTGCAGACAGGAGCCTGGCATGGCTGTCCTCTGAAAGGTTCCATCAGCAGCGGACTGAGACAGATGCAGATACTTACCTTATCACCAACCATTGGACTGAGCTCAGGAACCCTTATGGAAGAGTTAGGAGGAGAACTGAAGGAACTGAAGGGATTGCAACCCTATTGGAAGAACTAACCAGACACCTTCAGTGCTTTAAGAGACTAAGCCACCAACCAAAATGCATATATGGGCCTGTCCTTGCTCCCTGCACCCCCCAACTACACGTGTAACAGAGGACTCCCTTTTTCTGGCCTCAGTCAAAGGGATGTGCTTGGTCTTGCTGAGGCTTGATGCACCAGAGAAGGGAGATGTTAGAGGAGTGAGAAAAGGGGGAGTAATGGGGTGGGAAGTTCATGGAGGGGGTACCTGGAAGGAGAACAGCATTTGAAATATAAAAAAATAAAATTTTAAAAAGA\n",
      "seq GACCTCTTCGCTCTTGATGTGGAGCCCTACAGATACAGTGGCGTAAATATGACAGGGTTCAGAATACTAAATACAGAGAATACCCAAGTCTCCTCCATCATCGAGAAGTGGTCGATGGAACGGTTACAGGCACCTCCAAAACCTGACTCAGGTTTGCTGGATGGATTTATGACG\n",
      "seq GCATTAGCTATGGGAATGATGACAGAATACTACCACTATATATTTACGACTCTG\n",
      "seq GTCTCATTCGCTTGCAAGAGCTCATCAAAGCTCCATCAAGGTACAATCTTCGACTTAAAATTCGTCAGCTGCCAGCTGATACAAAAGATGCAAAGCCTTTGCTGAAAGAGATGAAGAGGGGCAAGGAGTTCCACGTGATCTTCGACTGCAGCCATGAAATGGCAGCAGGCATTTTAAAGCAG\n",
      "seq CTTGTGATCAGCTGTCTCTTGGGGTGGCTGCCATCTTCGGTCCTTCACACAGTTCATCAGCAAATGCTGTTCAGTCCATCTGCAATGCTCTGGGGGTTCCTCACATACAGACCCGCTGGAAGCACCAGGTGTCAGACAATAAGGATTCCTTCTATGTCAGTCTCTACCCAGACTTCTCTTCCCTCAGCCGTGCCATCTTGGATTTGGTGCAGTTTTTTAAGTGGAAAACTGTCACAGTTGTGTATGACGACAGCACTG\n",
      "seq GTGGTATATTTGAATATGTGGAATCTGGCCCTATGGGAGCTGAAGAACTTGCATTCAGATTTGCTGTGAATACAATCAACAGGAACAGGACTCTGCTACCCAATACCACGTTAACATATGATACACAGAAGATCAATCTCTATGACAGTTTTGAAGCATCTAAGAAAG\n",
      "seq GCTCGCGCGGCTGGACATTGTGCTTGCTGGATTTTTCCCGGATGCTCCCAGACTAACATGGATGTCCCACCATCCCTTGCAGTGGAAGCTTGCTCCTTGGCGCAGTGAGTGAAGAACATGCAGAGACTGCTAATGGGTTTGGGAAGCGGAGACTCCTTCCTCTTTCTGTGACCATGCCGTGATTGTGTTTGCGGCCACTATTCCACGCATCCTTCTTCTCGTCCAAGCCCGGAGCCTAACGCTAGATCGGGGAAGTGGGTGCCGCGCGCGCAGGCACGGAAACATCATGAAGATTATTTCCCCAGTTTTAAGTAATCTAGTCTTCAGTCGCTCCATTAAAGTCCTGCTCTGCTTGTTGTGGATCGGATATTCGCAAGGAACCACACATGTGTTAAGATTCG\n",
      "seq TCCGCCGCCGTCACTTGCGCTCTCGCTGGCGGCTGCGTGGCCGAGGCTGGGAGCCCGGGACTTCCCGCTGAACCGCCTCCTGCCGCAGCTCTGAGAGGACTACCCCAGTCCCTACCCTCCCTCTTCACCCTAGCCGCAG\n"
     ]
    }
   ],
   "source": [
    "selected_features = '../extract_features_output.txt'\n",
    "fasta_file = '../extract_seqs_output.fa'\n",
    "reference_fasta = '../data/transcriptome_out_tmp.fa'\n",
    "plp_length = 30\n",
    "min_coverage = 1\n",
    "output_file = 'Candidate_probes'\n",
    "gc_min = 50\n",
    "gc_max = 65\n",
    "num_probes = 10\n",
    "iupac_mismatches = None\n",
    "max_errors = 1\n",
    "check_specificity = False\n",
    "off_target_output = False\n",
    "\n",
    "targets = []\n",
    "\n",
    "# Read the FASTA file and selected features\n",
    "seq_dict = SeqIO.to_dict(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "selected_features = pd.read_csv(selected_features, sep='\\t')\n",
    "selected_features.index = selected_features['region']\n",
    "coverage_dict = dict(zip(selected_features['region'], selected_features['coverage']))\n",
    "\n",
    "if plp_length % 2 != 0:\n",
    "    raise InputValueError(\"The size of the probe should be an even number\", field=\"plp_length\", code=\"odd_value_for_plp_length_provided\")\n",
    "\n",
    "for key in seq_dict:\n",
    "    gene, region = key.split(\"|\")\n",
    "    chr_name, start_end = region.split(\":\")\n",
    "    initial_start = int(start_end.split(\"-\")[0])\n",
    "    seq = seq_dict[key].seq\n",
    "    seq_len = len(seq) - (plp_length - 1)\n",
    "    coverage_value = coverage_dict.get(region, 0)\n",
    "    if coverage_value < min_coverage:\n",
    "        continue \n",
    "\n",
    "    for i in range(seq_len):\n",
    "        tmp_seq = seq[i:i + plp_length]\n",
    "        gc_content = (tmp_seq.count(\"G\") + tmp_seq.count(\"C\")) / len(tmp_seq) * 100\n",
    "\n",
    "        if not (gc_min <= gc_content <= gc_max):\n",
    "            continue\n",
    "        if not any(nucleotide * 3 in tmp_seq for nucleotide in \"ACGT\"):\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Empty DataFrame\n",
       " Columns: [Ligation junction]\n",
       " Index: [],\n",
       " None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trgts_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
